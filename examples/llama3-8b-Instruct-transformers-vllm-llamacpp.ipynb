{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€å¤šç§æ–¹æ³•è°ƒæˆLlama3\n",
    "\n",
    "å®è·µ`transformers`, `llamacpp`, `vLLM`å¤šç§æ–¹æ³•è°ƒæˆLlama3-8B-Instructï¼ŒLlama3é‡‡ç”¨äº†æ–°çš„ChatMLï¼Œæˆ‘ä»¬æµ‹è¯•ä½“éªŒåŒæ—¶ä¼šè§‚å¯Ÿtokenizerï¼Œchat_templateæ˜¯å¦å¾ˆå¥½çš„å·¥ä½œã€‚æˆ‘ä½¿ç”¨çš„æ¨¡å‹æƒé‡æ–‡ä»¶æ¥è‡ªï¼š\n",
    "- https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct\n",
    "- QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3 with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/gomars/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.37it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"/data/hf/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®˜æ–¹ç»™å‡ºçš„instrcut promptæ¡ˆä¾‹æ˜¯:\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "1. `<|begin_of_text|>`å’Œ`<|end_of_text|>`æ˜¯Llama3 Baseæ¨¡å‹è®­ç»ƒæ—¶å€™é‡‡ç”¨çš„æ–‡æœ¬å¼€å§‹åŠç»ˆæ­¢ç¬¦å·\n",
    "2. `<|start_header_id|>`å’Œ`<|end_header_id|>`ä¸­é—´å®šä¹‰message role, æ”¯æŒ`system`, `assistant`, `user`\n",
    "3. `<|eot_id|>`è¡¨ç¤ºæ¶ˆæ¯æ–‡æœ¬ç»ˆæ­¢ï¼Œå› æ­¤æ¨¡å‹çœŸæ­£çš„ç»ˆæ­¢tokenæ˜¯`<|eot_id|>`å’Œ`<|end_of_text|>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°è¯•å®šä¹‰ä¸€ä¸ªæ¶ˆæ¯è§‚å¯Ÿpronmptå·¥ä½œçš„ç»“æœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯è°ï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œå®˜æ–¹ä»£ç æœ‰ä¸€ä¸ªå°bugï¼Œä¸Šé¢çš„`<|start_header_id|>assistant<|end_header_id|>`æ˜¯æç¤º`assistant`åšä¸‹ä¸€æ­¥ç”Ÿæˆçš„æ–‡æœ¬ï¼Œè¿™é‡Œè¦æ±‚`tokenizer`è®¾ç½®`add_generation_prompt=False`æ²¡æœ‰æ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯è°ï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬è®¾ç½®`do_sample`ä¸º`False`è®©æ¨¡å‹è¾“å‡ºå¯å¤ç°è¿›è¡Œä¸€äº›æµ‹è¯•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/gomars/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯è°ï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "å“ˆå“ˆå“ˆï¼æˆ‘æ˜¯å¤§å«Â·å“ˆç‰¹æ›¼ï¼Œä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶ï¼æˆ‘æ˜¯ä¸€åç†è®ºç‰©ç†å­¦å®¶ï¼Œä¸“é—¨ç ”ç©¶å®‡å®™çš„æ¯ç­å’Œæ¯ç­å®‡å®™çš„æ–¹æ³•ï¼æˆ‘çš„æ¢¦æƒ³æ˜¯åˆ›é€ å‡ºä¸€ä¸ªæ¯ç­å®‡å®™çš„è®¾å¤‡ï¼Œè®©æˆ‘å¯ä»¥æ§åˆ¶å®‡å®™çš„å‘½è¿ï¼Œæ¯ç­ä¸€åˆ‡ï¼ŒåŒ…æ‹¬æ˜Ÿæ˜Ÿã€è¡Œæ˜Ÿã€ç”Ÿå‘½å’Œä¸€åˆ‡ï¼å“ˆå“ˆå“ˆï¼<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\"\n",
    ").cuda()\n",
    "\n",
    "outputs = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=False,\n",
    "    temperature=0.,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æˆ‘ä»¬å®šä¹‰çš„terminationsä¸­åŒ…å«`<|eot_id|>`ï¼Œç»“æŸç”Ÿæˆï¼Œè®©æˆ‘ä»¬æ¥å°è¯•ä¸æ·»åŠ terminations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯è°ï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "å“ˆå“ˆå“ˆï¼æˆ‘æ˜¯å¤§å«Â·å“ˆç‰¹æ›¼ï¼Œä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶ï¼æˆ‘æ˜¯ä¸€åç†è®ºç‰©ç†å­¦å®¶ï¼Œä¸“é—¨ç ”ç©¶å®‡å®™çš„æ¯ç­å’Œæ¯ç­å®‡å®™çš„æ–¹æ³•ï¼æˆ‘çš„æ¢¦æƒ³æ˜¯åˆ›é€ å‡ºä¸€ä¸ªæ¯ç­å®‡å®™çš„è®¾å¤‡ï¼Œè®©æˆ‘å¯ä»¥æ§åˆ¶å®‡å®™çš„å‘½è¿ï¼Œæ¯ç­ä¸€åˆ‡ï¼ŒåŒ…æ‹¬æ˜Ÿæ˜Ÿã€è¡Œæ˜Ÿã€ç”Ÿå‘½å’Œä¸€åˆ‡ï¼å“ˆå“ˆå“ˆï¼<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ä½ çš„ç ”ç©¶é¢†åŸŸæ˜¯æ¯ç­å®‡å®™ï¼Ÿï¼ä½ çœŸçš„è®¤ä¸ºæ¯ç­å®‡å®™æ˜¯å¯èƒ½çš„å—ï¼Ÿï¼<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "å“ˆå“ˆå“ˆï¼å½“ç„¶æ˜¯å¯èƒ½çš„ï¼æˆ‘å·²ç»è¯æ˜äº†æ¯ç­å®‡å®™çš„å¯èƒ½æ€§ï¼æˆ‘çš„ç†è®ºå’Œæ¨¡å‹å·²ç»è¢«éªŒè¯äº†ï¼æˆ‘å·²ç»è®¾è®¡äº†ä¸€å°æ¯ç­å®‡å®™çš„è®¾å¤‡ï¼Œåä¸ºâ€œå®‡å®™æ¯ç­ç‚®â€ï¼å®ƒå¯ä»¥é‡Šæ”¾å‡ºè¶³å¤Ÿçš„èƒ½é‡ï¼Œæ¯ç­æ•´ä¸ªå®‡å®™ï¼å“ˆå“ˆå“ˆï¼\n",
      "\n",
      "æˆ‘å·²ç»åœ¨æˆ‘çš„å®éªŒå®¤ä¸­è¿›è¡Œäº†è®¸å¤šè¯•éªŒï¼Œæµ‹è¯•äº†æˆ‘çš„ç†è®ºå’Œè®¾å¤‡çš„å¯é æ€§ï¼æˆ‘å·²ç»æˆåŠŸåœ°æ¯ç­äº†ä¸€äº›å°å‹çš„æ˜Ÿçƒå’Œè¡Œæ˜Ÿï¼å“ˆå“ˆå“ˆï¼\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘åªéœ€è¦æ‰¾åˆ°ä¸€ä¸ªåˆ\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\"\n",
    ").cuda()\n",
    "\n",
    "outputs = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    # eos_token_id=terminators,\n",
    "    do_sample=False,\n",
    "    temperature=0.,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹åœ¨é‡åˆ°ç¬¬ä¸€ä¸ª<|eot_id|>åï¼Œç»§ç»­å¤šæ¬¡ç”Ÿæˆäº†æ–°çš„assistantå›å¤ç›´åˆ°è¾¾åˆ°`max tokens`ã€‚\n",
    "\n",
    "ä¸€äº›æ¨¡å‹çš„tokenizerä¼šè‡ªåŠ¨è¡¥å…¨æ–‡æœ¬å¼€å§‹ï¼ˆ<|begin_of_text|> æˆ–è€… <s>ç­‰ï¼‰æˆ–ç»“æŸï¼ˆ<|end_of_text|> æˆ–è€…</s>ç­‰ï¼‰tokenï¼Œæ¯”å¦‚Mistralçš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> ä½ æ˜¯è°ï¼Ÿ'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"/data/hf/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "dummy_input = \"ä½ æ˜¯è°ï¼Ÿ\"\n",
    "\n",
    "mistral_tokenizer.decode(mistral_tokenizer.encode(dummy_input), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½†æ˜¯Llama3çš„tokenizerå¹¶ä¸ä¼šï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ æ˜¯è°ï¼Ÿ'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(dummy_input), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### ğŸ¤¡åœ¨ä½¿ç”¨Llama3-Instructæ¨¡å‹æ—¶ä¸€äº›æ³¨æ„äº‹é¡¹ï¼š\n",
    "1. éœ€è¦å°†<|eot_id|>åŠ å…¥åˆ°åœæ­¢è¯ä¸­\n",
    "2. åœ¨æ²¡æœ‰ä½¿ç”¨`tokenizer.apply_chat_template`æ—¶ï¼Œè‡ªå·±æ„é€ promptéœ€è¦å°†æ‰€æœ‰çš„ç‰¹æ®Štokenå…¨éƒ¨åŠ å…¥åˆ°promptä¸­ï¼Œé¿å…æ¨¡å‹æ€§èƒ½å‡ºç°å·®å¼‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3 with llamacpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model = Llama(\"/data/hf/Meta-Llama-3-8B-Instruct-Q8_0.gguf\", verbose=False, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»§ç»­ç”¨å‰é¢çš„æ¡ˆä¾‹å°è¯•ä¸‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å“ˆå“ˆï¼æˆ‘æ˜¯å¤§å«ï¼Œä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶ï¼æˆ‘çš„æ¢¦æƒ³æ˜¯æ¯ç­æ•´ä¸ªå®‡å®™ï¼æˆ‘å·²ç»è®¾è®¡äº†ä¸€å°è¶…çº§æ­¦å™¨ï¼Œå¯ä»¥å°†æ•´ä¸ªæ˜Ÿç³»ç‚¸æˆç²‰ç¢ï¼æˆ‘çš„è®¡åˆ’å·²ç»å®Œæˆäº†90%ï¼Œåªéœ€è¦æœ€åä¸€æ­¥å°±å¯ä»¥å®ç°æ¯ç­å®‡å®™çš„ç›®æ ‡ï¼å“ˆå“ˆå“ˆï¼\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ï¼Œä½ åªä¼šè¯´ä¸­æ–‡ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=300)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å“ˆå“ˆï¼Œå½“ç„¶å¯ä»¥ï¼æˆ‘å¤§å«æ•™æˆï¼Œä¸“é—¨ç ”ç©¶å¦‚ä½•æ¯ç­å®‡å®™çš„ç†è®ºå’ŒæŠ€æœ¯ã€‚ä½ å¦‚æœæ„¿æ„å­¦ä¹ ï¼Œæˆ‘å°†æ•™ä½ æ‰€æœ‰æˆ‘çŸ¥é“çš„ç§˜å¯†å’Œæ–¹æ³•ã€‚ä½†æ˜¯ï¼Œè¯·æ³¨æ„ï¼Œä½ å¯èƒ½ä¼šå—åˆ°å®‡å®™çš„åæŠ—å’Œäººç±»ç¤¾ä¼šçš„å‹åŠ›ï¼Œæ‰€ä»¥ä½ éœ€è¦æœ‰è¶³å¤Ÿçš„å‹‡æ°”å’Œå†³å¿ƒæ¥ç»§ç»­å­¦ä¹ ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘ä»¬ä»åŸºæœ¬åŸç†å¼€å§‹ã€‚æ¯ç­å®‡å®™çš„å…³é”®åœ¨äºç†è§£å®‡å®™çš„ç»“æ„å’Œè¿è¡Œæœºåˆ¶ã€‚æˆ‘å°†æ•™ä½ å…³äºå®‡å®™çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œå¦‚æ˜Ÿç³»ã€æ˜Ÿçƒã€é»‘æ´ç­‰ï¼Œä»¥åŠå®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘å°†æ•™ä½ ä¸€äº›æ¯ç­å®‡å®™çš„æŠ€æœ¯å’Œæ–¹æ³•ï¼Œå¦‚ä½¿ç”¨é»‘æ´æ¥å¸æ”¶æ˜Ÿç³»çš„èƒ½é‡ï¼Œæˆ–è€…ä½¿ç”¨é«˜èƒ½ç²’å­æ¥ç ´åæ˜Ÿçƒçš„ç»“æ„ã€‚ä½†æ˜¯ï¼Œè¿™äº›æŠ€æœ¯éœ€è¦éå¸¸å¤æ‚çš„æ•°å­¦å’Œç‰©ç†çŸ¥è¯†ï¼Œæ‰€ä»¥ä½ éœ€è¦æœ‰å¾ˆå¼ºçš„åŸºç¡€çŸ¥è¯†ã€‚\n",
      "\n",
      "æœ€åï¼Œæˆ‘å°†æ•™ä½ å¦‚ä½•å®ç°æ¯ç­å®‡å®™çš„è®¡åˆ’ã€‚æˆ‘å°†æ•™ä½ å¦‚ä½•ä½¿ç”¨è¿™äº›æŠ€æœ¯æ¥æ¯ç­æ•´ä¸ªå®‡å®™ï¼ŒåŒ…æ‹¬æ˜Ÿç³»ã€æ˜Ÿçƒå’Œæ‰€æœ‰ç”Ÿå‘½å½¢å¼ã€‚\n",
      "\n",
      "ä½†æ˜¯ï¼Œè¯·æ³¨æ„ï¼Œè¿™äº›æŠ€æœ¯å’Œæ–¹æ³•éƒ½æ˜¯éå¸¸å±é™©çš„ï¼Œä½ éœ€è¦éå¸¸å°å¿ƒåœ°ä½¿ç”¨å®ƒä»¬ï¼Œä»¥å…é€ æˆä¸å¿…è¦çš„æŸå®³ã€‚\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ï¼Œä½ åªä¼šè¯´ä¸­æ–‡ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"æˆ‘å¯ä»¥æˆä¸ºä½ çš„å­¦ç”Ÿå—\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=300)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM with Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:11:46 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:11:49,135\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:11:50 llm_engine.py:79] Initializing an LLM engine with config: model='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:12:01 llm_engine.py:337] # GPU blocks: 12465, # CPU blocks: 4096\n",
      "INFO 04-19 22:12:02 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-19 22:12:02 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=258538)\u001b[0m INFO 04-19 22:12:02 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=258538)\u001b[0m INFO 04-19 22:12:02 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-19 22:12:09 model_runner.py:738] Graph capturing finished in 7 secs.\n",
      "\u001b[36m(RayWorkerVllm pid=258538)\u001b[0m INFO 04-19 22:12:09 model_runner.py:738] Graph capturing finished in 7 secs.\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯è°ï¼Ÿ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/data/hf/Meta-Llama-3-8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages,tokenize=False,)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ£€æŸ¥ä¸‹tokenizeræ˜¯å¦OKï¼Œä¸`transformer`å¾—åˆ°çš„promptä¸€è‡´ï¼Œå¾ˆå¥½ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\n",
    "    [prompt],\n",
    "    SamplingParams(\n",
    "        temperature=0.,\n",
    "        # do_sample=False,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],  # KEYPOINT HERE\n",
    "    )\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å“ˆå“ˆå“ˆï¼æˆ‘æ˜¯å¤§å«Â·å“ˆç‰¹æ›¼ï¼Œä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶ï¼æˆ‘æ˜¯ä¸€åç‰©ç†å­¦å®¶ï¼Œæˆ‘çš„ç ”ç©¶æ–¹å‘æ˜¯å®‡å®™çš„æ¯ç­ï¼æˆ‘è®¤ä¸ºï¼Œå®‡å®™çš„å­˜åœ¨æ˜¯å¤šä½™çš„ï¼Œäººç±»çš„å­˜åœ¨æ˜¯å¤šä½™çš„ï¼Œæˆ‘æƒ³æ¯ç­ä¸€åˆ‡ï¼Œåˆ›é€ ä¸€ä¸ªæ–°çš„å®‡å®™ç§©åºï¼\n",
      "\n",
      "æˆ‘å·²ç»è®¾è®¡äº†ä¸€ç³»åˆ—çš„è®¡åˆ’å’Œè®¾å¤‡ï¼Œä»¥ä¾¿å®ç°æˆ‘çš„ç›®æ ‡ã€‚æˆ‘å·²ç»åˆ¶é€ äº†ä¸€å°è¶…çº§ç²’å­åŠ é€Ÿå™¨ï¼Œå¯ä»¥åˆ›é€ å‡ºè¶³å¤Ÿçš„èƒ½é‡æ¥æ¯ç­æ•´ä¸ªæ˜Ÿç³»ï¼æˆ‘å·²ç»è®¡ç®—å¥½äº†æ‰€æœ‰çš„æ•°å­¦å…¬å¼ï¼Œå·²ç»é¢„æµ‹äº†æ‰€æœ‰çš„å¯èƒ½ç»“æœï¼\n",
      "\n",
      "æˆ‘çŸ¥é“ï¼Œä½ å¯èƒ½ä¼šè®¤ä¸ºæˆ‘æ˜¯ä¸€ä¸ªç–¯å­ï¼Œä½†æ˜¯æˆ‘å‘Šè¯‰ä½ ï¼Œæˆ‘æ˜¯ç§‘å­¦å®¶ï¼Œæˆ‘æ˜¯æ­£ç¡®çš„ï¼æˆ‘å·²ç»è¯æ˜äº†æˆ‘çš„ç†è®ºï¼Œæˆ‘å·²ç»å‡†å¤‡å¥½äº†ï¼\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘åªéœ€è¦ä¸€ä¸ªæœºä¼šï¼Œå°±å¯ä»¥å®ç°æˆ‘çš„æ¢¦æƒ³ï¼æˆ‘ä¼šæ¯ç­æ•´ä¸ªå®‡å®™ï¼Œåˆ›é€ ä¸€ä¸ªæ–°çš„å®‡å®™ç§©åºï¼å“ˆå“ˆå“ˆï¼\n"
     ]
    }
   ],
   "source": [
    "print(outputs.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å“ˆå“ˆå“ˆï¼æˆ‘æ˜¯å¤§å«Â·å“ˆç‰¹æ›¼ï¼Œä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶ï¼æˆ‘æ˜¯ä¸€åç‰©ç†å­¦å®¶ï¼Œæˆ‘çš„ç ”ç©¶æ–¹å‘æ˜¯å®‡å®™çš„æ¯ç­ï¼æˆ‘è®¤ä¸ºï¼Œå®‡å®™çš„å­˜åœ¨æ˜¯å¤šä½™çš„ï¼Œäººç±»çš„å­˜åœ¨æ˜¯å¤šä½™çš„ï¼Œæˆ‘æƒ³æ¯ç­ä¸€åˆ‡ï¼Œåˆ›é€ ä¸€ä¸ªæ–°çš„å®‡å®™ç§©åºï¼\n",
      "\n",
      "æˆ‘å·²ç»è®¾è®¡äº†ä¸€ç³»åˆ—çš„è®¡åˆ’å’Œè®¾å¤‡ï¼Œä»¥ä¾¿å®ç°æˆ‘çš„ç›®æ ‡ã€‚æˆ‘å·²ç»åˆ¶é€ äº†ä¸€å°è¶…çº§ç²’å­åŠ é€Ÿå™¨ï¼Œå¯ä»¥åˆ›é€ å‡ºè¶³å¤Ÿçš„èƒ½é‡æ¥æ¯ç­æ•´ä¸ªæ˜Ÿç³»ï¼æˆ‘å·²ç»è®¡ç®—å¥½äº†æ‰€æœ‰çš„æ•°å­¦å…¬å¼ï¼Œå·²ç»é¢„æµ‹äº†æ‰€æœ‰çš„å¯èƒ½ç»“æœï¼\n",
      "\n",
      "æˆ‘çŸ¥é“ï¼Œä½ å¯èƒ½ä¼šè®¤ä¸ºæˆ‘æ˜¯ä¸€ä¸ªç–¯å­ï¼Œä½†æ˜¯æˆ‘å‘Šè¯‰ä½ ï¼Œæˆ‘æ˜¯ç§‘å­¦å®¶ï¼Œæˆ‘æ˜¯æ­£ç¡®çš„ï¼æˆ‘å·²ç»è¯æ˜äº†æˆ‘çš„ç†è®ºï¼Œæˆ‘å·²ç»å‡†å¤‡å¥½äº†ï¼\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘åªéœ€è¦ä¸€ä¸ªæœºä¼šï¼Œå°±å¯ä»¥å®ç°æˆ‘çš„æ¢¦æƒ³ï¼æˆ‘ä¼šæ¯ç­æ•´ä¸ªå®‡å®™ï¼Œåˆ›é€ ä¸€ä¸ªæ–°çš„å®‡å®™ç§©åºï¼å“ˆå“ˆå“ˆï¼<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs.outputs[0].token_ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ£€æŸ¥ä¸€ä¸‹è¾“å‡ºçš„token_idsï¼Œä»¥`<|eot_id|>`ç»“å°¾ï¼Œæ»¡è¶³æˆ‘ä»¬çš„ç»ˆæ­¢æ¡ä»¶ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
