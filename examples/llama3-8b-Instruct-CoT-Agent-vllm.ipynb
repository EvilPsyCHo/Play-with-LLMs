{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Meta-Llama3-8b-Instruct CoT Agent with vLLM\n",
    "\n",
    "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n",
    "\n",
    "LLM本质上是自回归模型，下一个token预测依赖之前的上下文，通过给予一些token空间让模型对问题做进一步拆在，一步步思考最后得到结论，可以显著提升模型解决复杂问题的准确率，这就CoT(Chain of Thoughts)的核心。在[mistral-CoT-Agent](./mistral-CoT-Agent.ipynb)案例中，我们利用GBNF对语言模型输出进行了强制规范，这次我们测试一下`Llama3-8b-Instruct`模型的能力：\n",
    "1. 是否能够自然的支持CoT场景\n",
    "2. 对中文prompt的支持能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/gomars/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-19 22:40:58,774\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:40:58 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:41:00,727\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:41:01 llm_engine.py:79] Initializing an LLM engine with config: model='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:41:12 llm_engine.py:337] # GPU blocks: 12465, # CPU blocks: 4096\n",
      "INFO 04-19 22:41:14 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-19 22:41:14 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=263474)\u001b[0m INFO 04-19 22:41:14 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=263474)\u001b[0m INFO 04-19 22:41:14 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-19 22:41:20 model_runner.py:738] Graph capturing finished in 6 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=263474)\u001b[0m INFO 04-19 22:41:20 model_runner.py:738] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/data/hf/Meta-Llama-3-8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"自助餐厅有23个苹果。 如果他们用 20 个苹果做午餐，然后又买了 6 个，他们有多少个苹果？\"\n",
    "system_prompt = '''你是一个卓越的AI助手，一步步思考回答用户的问题，回答格式遵循：\n",
    "思考：将你的思考过程写在这里。\n",
    "回答：将你的最终答案放在这里。\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages,tokenize=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\n",
    "    [prompt],\n",
    "    SamplingParams(\n",
    "        temperature=0.,\n",
    "        # do_sample=False,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],  # KEYPOINT HERE\n",
    "    )\n",
    ")[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "思考：\n",
      "\n",
      "首先，我们需要知道自助餐厅原来有23个苹果。然后，他们用20个苹果做午餐，这意味着他们已经使用了20个苹果。剩下的苹果数量是23 - 20 = 3个。\n",
      "\n",
      "接着，他们又买了6个苹果，这意味着他们现在拥有3 + 6 = 9个苹果。\n",
      "\n",
      "回答：\n",
      "\n",
      "自助餐厅现在有9个苹果。\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完全正确，格式满足要求，同时对中文prompt的理解以及中文输出能力都在水准以上。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
