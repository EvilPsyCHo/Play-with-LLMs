{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Meta-Llama3-8b-Instruct CoT Agent with vLLM\n",
    "\n",
    "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n",
    "\n",
    "LLMæœ¬è´¨ä¸Šæ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œä¸‹ä¸€ä¸ªtokené¢„æµ‹ä¾èµ–ä¹‹å‰çš„ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡ç»™äºˆä¸€äº›tokenç©ºé—´è®©æ¨¡å‹å¯¹é—®é¢˜åšè¿›ä¸€æ­¥æ‹†åœ¨ï¼Œä¸€æ­¥æ­¥æ€è€ƒæœ€åå¾—åˆ°ç»“è®ºï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹è§£å†³å¤æ‚é—®é¢˜çš„å‡†ç¡®ç‡ï¼Œè¿™å°±CoT(Chain of Thoughts)çš„æ ¸å¿ƒã€‚åœ¨[mistral-CoT-Agent](./mistral-CoT-Agent.ipynb)æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨GBNFå¯¹è¯­è¨€æ¨¡å‹è¾“å‡ºè¿›è¡Œäº†å¼ºåˆ¶è§„èŒƒï¼Œè¿™æ¬¡æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹`Llama3-8b-Instruct`æ¨¡å‹çš„èƒ½åŠ›ï¼š\n",
    "1. æ˜¯å¦èƒ½å¤Ÿè‡ªç„¶çš„æ”¯æŒCoTåœºæ™¯\n",
    "2. å¯¹ä¸­æ–‡promptçš„æ”¯æŒèƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/gomars/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-19 22:40:58,774\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:40:58 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:41:00,727\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:41:01 llm_engine.py:79] Initializing an LLM engine with config: model='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 22:41:12 llm_engine.py:337] # GPU blocks: 12465, # CPU blocks: 4096\n",
      "INFO 04-19 22:41:14 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-19 22:41:14 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=263474)\u001b[0m INFO 04-19 22:41:14 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=263474)\u001b[0m INFO 04-19 22:41:14 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-19 22:41:20 model_runner.py:738] Graph capturing finished in 6 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=263474)\u001b[0m INFO 04-19 22:41:20 model_runner.py:738] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/data/hf/Meta-Llama-3-8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"è‡ªåŠ©é¤å…æœ‰23ä¸ªè‹¹æœã€‚ å¦‚æœä»–ä»¬ç”¨ 20 ä¸ªè‹¹æœåšåˆé¤ï¼Œç„¶ååˆä¹°äº† 6 ä¸ªï¼Œä»–ä»¬æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\"\n",
    "system_prompt = '''ä½ æ˜¯ä¸€ä¸ªå“è¶Šçš„AIåŠ©æ‰‹ï¼Œä¸€æ­¥æ­¥æ€è€ƒå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå›ç­”æ ¼å¼éµå¾ªï¼š\n",
    "æ€è€ƒï¼šå°†ä½ çš„æ€è€ƒè¿‡ç¨‹å†™åœ¨è¿™é‡Œã€‚\n",
    "å›ç­”ï¼šå°†ä½ çš„æœ€ç»ˆç­”æ¡ˆæ”¾åœ¨è¿™é‡Œã€‚\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages,tokenize=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\n",
    "    [prompt],\n",
    "    SamplingParams(\n",
    "        temperature=0.,\n",
    "        # do_sample=False,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],  # KEYPOINT HERE\n",
    "    )\n",
    ")[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€è€ƒï¼š\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“è‡ªåŠ©é¤å…åŸæ¥æœ‰23ä¸ªè‹¹æœã€‚ç„¶åï¼Œä»–ä»¬ç”¨20ä¸ªè‹¹æœåšåˆé¤ï¼Œè¿™æ„å‘³ç€ä»–ä»¬å·²ç»ä½¿ç”¨äº†20ä¸ªè‹¹æœã€‚å‰©ä¸‹çš„è‹¹æœæ•°é‡æ˜¯23 - 20 = 3ä¸ªã€‚\n",
      "\n",
      "æ¥ç€ï¼Œä»–ä»¬åˆä¹°äº†6ä¸ªè‹¹æœï¼Œè¿™æ„å‘³ç€ä»–ä»¬ç°åœ¨æ‹¥æœ‰3 + 6 = 9ä¸ªè‹¹æœã€‚\n",
      "\n",
      "å›ç­”ï¼š\n",
      "\n",
      "è‡ªåŠ©é¤å…ç°åœ¨æœ‰9ä¸ªè‹¹æœã€‚\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®Œå…¨æ­£ç¡®ï¼Œæ ¼å¼æ»¡è¶³è¦æ±‚ï¼ŒåŒæ—¶å¯¹ä¸­æ–‡promptçš„ç†è§£ä»¥åŠä¸­æ–‡è¾“å‡ºèƒ½åŠ›éƒ½åœ¨æ°´å‡†ä»¥ä¸Šã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
