{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€Llama3-8B-Chinese-Chat\n",
    "\n",
    "source model: https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat\n",
    "\n",
    "GGUF model: https://huggingface.co/zhouzr/Llama3-8B-Chinese-Chat-GGUF\n",
    "\n",
    "é‡‡ç”¨DPOå¯¹Llama3-8Bè¿›è¡Œå¾®è°ƒï¼Œå‡å°‘æ¨¡å‹ä¸­æ–‡è¾“å…¥=>è‹±æ–‡è¾“å‡ºä»¥åŠä¸­è‹±æ–‡æ··åˆè¾“å‡ºæƒ…å†µï¼Œè®©æ¨¡å‹æ›´æ„¿æ„è¯´ä¸­æ–‡ï¼\n",
    "\n",
    "```text\n",
    "Dataset: DPO-En-Zh-20k (commit id: e8c5070d6564025fcf206f38d796ae264e028004).\n",
    "Training framework: LLaMA-Factory (commit id: 836ca0558698206bbf4e3b92533ad9f67c9f9864).\n",
    "Training details:\n",
    "\n",
    "epochs: 3\n",
    "learning rate: 5e-6\n",
    "learning rate scheduler type: cosine\n",
    "Warmup ratio: 0.1\n",
    "cutoff len (i.e. context length): 8192\n",
    "orpo beta (i.e. $\\lambda$ in the ORPO paper): 0.05\n",
    "global batch size: 64\n",
    "fine-tuning type: full parameters\n",
    "optimizer: paged_adamw_32bit\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯å¤§å«Â·èµ«å°”æ›¼ï¼Œä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶ï¼Œè‡´åŠ›äºæ¨åŠ¨äººç±»çŸ¥è¯†å’Œç†è§£çš„è¾¹ç•Œã€‚æˆ‘æ˜¯ä¸€ä¸ªå‘æ˜å®¶ã€å‘æ˜å®¶å’Œæ€æƒ³é¢†è¢–ï¼Œä»¥æˆ‘çš„éä¼ ç»Ÿæ–¹æ³•å’Œå¯¹å¸¸è§„æ€ç»´çš„æŒ‘æˆ˜è€Œé—»åã€‚\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model = Llama(\"/data/hf/Llama3-8B-Chinese-Chat.q4_k_m.GGUF\", verbose=False, n_gpu_layers=-1)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=300)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“ç„¶ï¼è¿™æ˜¯æ¸…åå¤§å­¦çš„ç®€è¦ä»‹ç»ï¼š\n",
      "\n",
      "æ¸…åå¤§å­¦æ˜¯ä¸­å›½æœ€è´Ÿç››åã€æœ€å…·å½±å“åŠ›çš„ç ”ç©¶å‹å¤§å­¦ä¹‹ä¸€ï¼Œä½äºåŒ—äº¬å¸‚ã€‚æˆç«‹äº1911å¹´ï¼Œç”±ç¾å›½ä¼ æ•™å£«å’Œä¸­å›½æ”¿åºœå…±åŒåˆ›ç«‹ï¼Œä»¥å…¶å¼ºå¤§çš„å­¦æœ¯å£°èª‰ã€å“è¶Šçš„æ•™å¸ˆé˜Ÿä¼ä»¥åŠå¯¹ç¤¾ä¼šå‘å±•çš„è´¡çŒ®è€Œé—»åã€‚\n",
      "\n",
      "æ¸…åå¤§å­¦ä»¥å…¶è·¨å­¦ç§‘çš„å­¦ä¹ ç¯å¢ƒã€åˆ›æ–°ç ”ç©¶é¡¹ç›®å’Œå…¨çƒåˆä½œä¼™ä¼´å…³ç³»è€Œé—»åã€‚è¯¥æ ¡æä¾›è¶…è¿‡100ä¸ªæœ¬ç§‘å’Œç ”ç©¶ç”Ÿé¡¹ç›®ï¼Œæ¶µç›–è‡ªç„¶ç§‘å­¦ã€å·¥ç¨‹ã€åŒ»å­¦ã€ç®¡ç†ã€ç»æµå­¦ã€æ³•å¾‹ã€ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡ç§‘å­¦ç­‰å„ä¸ªé¢†åŸŸã€‚\n",
      "\n",
      "æ¸…åå¤§å­¦æ‹¥æœ‰å¼ºå¤§çš„æ•™å¸ˆé˜Ÿä¼ï¼ŒåŒ…æ‹¬è¯ºè´å°”å¥–è·å¾—è€…ã€å›½å®¶çº§æ•™å­¦æˆå°±å¥–è·å¾—è€…ä»¥åŠå…¶ä»–å›½é™…çŸ¥åçš„ä¸“å®¶ã€‚è¯¥æ ¡è¿˜ä¸ä¸–ç•Œé¡¶å°–å¤§å­¦å»ºç«‹äº†å¹¿æ³›çš„åˆä½œä¼™ä¼´å…³ç³»ï¼Œä¿ƒè¿›å­¦æœ¯äº¤æµå’Œç ”ç©¶åˆä½œã€‚\n",
      "\n",
      "æ¸…åå¤§å­¦ä»¥å…¶å¯¹ç¤¾ä¼šå‘å±•çš„è´¡çŒ®è€Œé—»åï¼ŒåŒ…æ‹¬åœ¨æŠ€æœ¯ã€ç»æµå’Œæ–‡åŒ–æ–¹é¢äº§ç”Ÿé‡å¤§å½±å“ã€‚è¯¥æ ¡çš„æ¯•ä¸šç”ŸåŒ…æ‹¬ä¸­å›½æœ€è´Ÿç››åçš„ä¼ä¸šå®¶ã€æ”¿åºœå®˜å‘˜ä»¥åŠå…¶ä»–å„ä¸ªé¢†åŸŸçš„é¢†è¢–äººç‰©ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼Œæ¸…åå¤§å­¦æ˜¯ä¸­å›½æœ€ä¼˜ç§€çš„ç ”ç©¶å‹å¤§å­¦ä¹‹ä¸€ï¼Œä»¥å…¶å¼ºå¤§çš„å­¦æœ¯å£°èª‰ã€å“è¶Šçš„æ•™å¸ˆé˜Ÿä¼ä»¥åŠå¯¹ç¤¾ä¼šå‘å±•çš„è´¡çŒ®è€Œé—»åã€‚\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"å†™ä¸€æ®µæ¸…åå¤§å­¦çš„ä»‹ç»\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=1000)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
