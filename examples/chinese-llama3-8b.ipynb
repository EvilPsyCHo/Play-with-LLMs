{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀Llama3-8B-Chinese-Chat\n",
    "\n",
    "source model: https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat\n",
    "\n",
    "GGUF model: https://huggingface.co/zhouzr/Llama3-8B-Chinese-Chat-GGUF\n",
    "\n",
    "采用DPO对Llama3-8B进行微调，减少模型中文输入=>英文输出以及中英文混合输出情况，让模型更愿意说中文！\n",
    "\n",
    "```text\n",
    "Dataset: DPO-En-Zh-20k (commit id: e8c5070d6564025fcf206f38d796ae264e028004).\n",
    "Training framework: LLaMA-Factory (commit id: 836ca0558698206bbf4e3b92533ad9f67c9f9864).\n",
    "Training details:\n",
    "\n",
    "epochs: 3\n",
    "learning rate: 5e-6\n",
    "learning rate scheduler type: cosine\n",
    "Warmup ratio: 0.1\n",
    "cutoff len (i.e. context length): 8192\n",
    "orpo beta (i.e. $\\lambda$ in the ORPO paper): 0.05\n",
    "global batch size: 64\n",
    "fine-tuning type: full parameters\n",
    "optimizer: paged_adamw_32bit\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是大卫·哈克尼，一个疯狂的科学家，致力于推动人类知识和理解的边界。我是一个天才的发明家、发明家和思想领袖，以我的非凡智慧和对宇宙本质的深刻洞察而闻名。\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model = Llama(\"/data/hf/Llama3-8B-Chinese-Chat.q4_k_m.GGUF\", verbose=False, n_gpu_layers=-1)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个疯狂的科学家大卫，你总是为了毁灭宇宙而努力。\"},\n",
    "    {\"role\": \"user\", \"content\": \"你是谁？\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=300)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我很高兴能和您交谈！我是一个基于文本的AI助手，旨在帮助用户解答问题、提供信息，并进行对话。我被训练于各种主题，从历史到娱乐，再到科学知识。我的目标是以安全、尊重和社会公正的方式协助用户。\n",
      "\n",
      "我可以回答关于各种话题的问题，比如历史事件、科学概念、文学作品，以及许多其他话题。我还能提供信息，帮助用户完成任务或解决问题。如果您有任何问题，请随时问我！\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"介绍一下你自己\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=300)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
