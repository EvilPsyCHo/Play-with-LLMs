{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€Llama3-8B-Chinese-Chat\n",
    "\n",
    "source model: https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat\n",
    "\n",
    "GGUF model: https://huggingface.co/zhouzr/Llama3-8B-Chinese-Chat-GGUF\n",
    "\n",
    "é‡‡ç”¨DPOå¯¹Llama3-8Bè¿›è¡Œå¾®è°ƒï¼Œå‡å°‘æ¨¡å‹ä¸­æ–‡è¾“å…¥=>è‹±æ–‡è¾“å‡ºä»¥åŠä¸­è‹±æ–‡æ··åˆè¾“å‡ºæƒ…å†µï¼Œè®©æ¨¡å‹æ›´æ„¿æ„è¯´ä¸­æ–‡ï¼\n",
    "\n",
    "```text\n",
    "Dataset: DPO-En-Zh-20k (commit id: e8c5070d6564025fcf206f38d796ae264e028004).\n",
    "Training framework: LLaMA-Factory (commit id: 836ca0558698206bbf4e3b92533ad9f67c9f9864).\n",
    "Training details:\n",
    "\n",
    "epochs: 3\n",
    "learning rate: 5e-6\n",
    "learning rate scheduler type: cosine\n",
    "Warmup ratio: 0.1\n",
    "cutoff len (i.e. context length): 8192\n",
    "orpo beta (i.e. $\\lambda$ in the ORPO paper): 0.05\n",
    "global batch size: 64\n",
    "fine-tuning type: full parameters\n",
    "optimizer: paged_adamw_32bit\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯å¤§å«Â·å“ˆå…‹å°¼ï¼Œä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶ï¼Œè‡´åŠ›äºæ¨åŠ¨äººç±»çŸ¥è¯†å’Œç†è§£çš„è¾¹ç•Œã€‚æˆ‘æ˜¯ä¸€ä¸ªå¤©æ‰çš„å‘æ˜å®¶ã€å‘æ˜å®¶å’Œæ€æƒ³é¢†è¢–ï¼Œä»¥æˆ‘çš„éå‡¡æ™ºæ…§å’Œå¯¹å®‡å®™æœ¬è´¨çš„æ·±åˆ»æ´å¯Ÿè€Œé—»åã€‚\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model = Llama(\"/data/hf/Llama3-8B-Chinese-Chat.q4_k_m.GGUF\", verbose=False, n_gpu_layers=-1)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„ç§‘å­¦å®¶å¤§å«ï¼Œä½ æ€»æ˜¯ä¸ºäº†æ¯ç­å®‡å®™è€ŒåŠªåŠ›ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=300)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘å¾ˆé«˜å…´èƒ½å’Œæ‚¨äº¤è°ˆï¼æˆ‘æ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„AIåŠ©æ‰‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œå¯¹è¯ã€‚æˆ‘è¢«è®­ç»ƒäºå„ç§ä¸»é¢˜ï¼Œä»å†å²åˆ°å¨±ä¹ï¼Œå†åˆ°ç§‘å­¦çŸ¥è¯†ã€‚æˆ‘çš„ç›®æ ‡æ˜¯ä»¥å®‰å…¨ã€å°Šé‡å’Œç¤¾ä¼šå…¬æ­£çš„æ–¹å¼ååŠ©ç”¨æˆ·ã€‚\n",
      "\n",
      "æˆ‘å¯ä»¥å›ç­”å…³äºå„ç§è¯é¢˜çš„é—®é¢˜ï¼Œæ¯”å¦‚å†å²äº‹ä»¶ã€ç§‘å­¦æ¦‚å¿µã€æ–‡å­¦ä½œå“ï¼Œä»¥åŠè®¸å¤šå…¶ä»–è¯é¢˜ã€‚æˆ‘è¿˜èƒ½æä¾›ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡æˆ–è§£å†³é—®é¢˜ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"},\n",
    "]\n",
    "\n",
    "output = model.create_chat_completion(messages, stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], max_tokens=300)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
