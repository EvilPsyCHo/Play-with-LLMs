{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀Llama3-8b-Instruct React Agent\n",
    "\n",
    "实现过程类似[mistral-ReAct-Agent-with-function-tool-call](./mistral-ReAct-Agent-with-function-tool-call)案例，主要区别是：\n",
    "0. 模型从Mixtral-8x22b-Instruct切换到Llama3-8b-Instruct\n",
    "1. 尝试用中文定义Prompt观察效果\n",
    "2. 采用vLLM而不是llamacpp作为推理后端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/gomars/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-19 23:01:06,294\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 23:01:06 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 23:01:08,243\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 23:01:09 llm_engine.py:79] Initializing an LLM engine with config: model='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 23:01:20 llm_engine.py:337] # GPU blocks: 12465, # CPU blocks: 4096\n",
      "INFO 04-19 23:01:21 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-19 23:01:21 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=273778)\u001b[0m INFO 04-19 23:01:21 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=273778)\u001b[0m INFO 04-19 23:01:21 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-19 23:01:27 model_runner.py:738] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=273778)\u001b[0m INFO 04-19 23:01:27 model_runner.py:738] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/data/hf/Meta-Llama-3-8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_weather(city: str):\n",
    "    \"\"\"一个输入城市名称查询天气的函数\n",
    "    \n",
    "    Args:\n",
    "        city: 你想查询的城市名称\n",
    "    \"\"\"\n",
    "    if city == \"北京\":\n",
    "        return \"北京气温36度\"\n",
    "    elif city == \"巴黎\":\n",
    "        return \"巴黎气温24度\"\n",
    "    else:\n",
    "        return f\"{city}气温20度\"\n",
    "\n",
    "func_desc = \"\"\"function: search_weather\n",
    "  description: 一个输入城市名称查询天气的函数\n",
    "  args:\n",
    "    city (str): 你想查询的城市名称\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_PROMPT = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "你是一个AI智能，能够使用各种函数来回答问题。\n",
    "\n",
    "### 可以访问的函数\n",
    "\n",
    "{{func_desc}}\n",
    "\n",
    "### 回复格式\n",
    "使用如下格式进行回复：\n",
    "\n",
    "思考： 一步步思考解决问题，将你思考的过程放在这里\n",
    "行动：\n",
    "```json\n",
    "{\n",
    "    \"function\": $FUNCTION_NAME,\n",
    "    \"args\": $FUNCTION_ARGS\n",
    "}\n",
    "```\n",
    "观察: 得到函数返回结果\n",
    "...(这里思考/行动/观察可以重复n次) \n",
    "思考：现在我知道最终答案了\n",
    "答案：写出最终答案\n",
    "\n",
    "$FUNCTION_NAME 是函数名. $FUNCTION_ARGS 是复合函数要求的字典输入。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "问题：{{question}} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "''' # 这里最后一定要添加两个\\n\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import Dict, List\n",
    "import re\n",
    "import json\n",
    "\n",
    "def react_agent(model, question, function, function_desc, max_rounds=3):\n",
    "    print(f\"Question: {question}\")\n",
    "    prompt = REACT_PROMPT.replace(\"{{question}}\", question).replace(\"{{func_desc}}\", function_desc)\n",
    "    \n",
    "    output = \"\"\n",
    "    try:\n",
    "        for i in range(max_rounds):\n",
    "            # Thought step\n",
    "            response = llm.generate([prompt], sampling_params=SamplingParams(temperature=0., max_tokens=1024, \n",
    "                                                                             stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],  # KEYPOINT HERE\n",
    "                                                                             stop=[\"观察：\"]))[0].outputs[0].text\n",
    "            output += response\n",
    "            prompt += response\n",
    "            print(response, end=\"\")\n",
    "            # If \"FinalAnswer\" in reponse, end react process\n",
    "            if \"答案：\" in response:\n",
    "                output += response\n",
    "                break\n",
    "\n",
    "            elif \"行动：\" in response:\n",
    "                args = json.loads(re.findall(\"```json([\\s\\S]*?)```\", response)[0].replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "                obs = function(**args[\"args\"])\n",
    "                obs = f\"观察：{obs}\" + \"\\n思考：\"\n",
    "                output += obs\n",
    "                prompt += obs\n",
    "                print(obs, end=\"\")\n",
    "                continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 北京和巴黎现在哪个地方更热？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "思考：首先，我需要知道北京和巴黎现在的天气情况。\n",
      "\n",
      "行动：\n",
      "```json\n",
      "{\n",
      "    \"function\": \"search_weather\",\n",
      "    \"args\": {\n",
      "        \"city\": \"北京\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "观察：北京气温36度\n",
      "思考："
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "然后，我需要知道巴黎现在的天气情况。\n",
      "\n",
      "行动：\n",
      "```json\n",
      "{\n",
      "    \"function\": \"search_weather\",\n",
      "    \"args\": {\n",
      "        \"city\": \"巴黎\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "观察：巴黎气温24度\n",
      "思考："
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在我知道了北京和巴黎的天气情况，可以比较气温来确定哪个地方更热。\n",
      "\n",
      "答案：北京更热。"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"北京和巴黎现在哪个地方更热？\"\n",
    "output = react_agent(llm, question, search_weather, func_desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
