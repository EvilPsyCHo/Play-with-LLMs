{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€Llama3-8b-Instruct React Agent\n",
    "\n",
    "å®ç°è¿‡ç¨‹ç±»ä¼¼[mistral-ReAct-Agent-with-function-tool-call](./mistral-ReAct-Agent-with-function-tool-call)æ¡ˆä¾‹ï¼Œä¸»è¦åŒºåˆ«æ˜¯ï¼š\n",
    "0. æ¨¡å‹ä»Mixtral-8x22b-Instructåˆ‡æ¢åˆ°Llama3-8b-Instruct\n",
    "1. å°è¯•ç”¨ä¸­æ–‡å®šä¹‰Promptè§‚å¯Ÿæ•ˆæœ\n",
    "2. é‡‡ç”¨vLLMè€Œä¸æ˜¯llamacppä½œä¸ºæ¨ç†åç«¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/gomars/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-19 23:01:06,294\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 23:01:06 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 23:01:08,243\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 23:01:09 llm_engine.py:79] Initializing an LLM engine with config: model='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-19 23:01:20 llm_engine.py:337] # GPU blocks: 12465, # CPU blocks: 4096\n",
      "INFO 04-19 23:01:21 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-19 23:01:21 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=273778)\u001b[0m INFO 04-19 23:01:21 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=273778)\u001b[0m INFO 04-19 23:01:21 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-19 23:01:27 model_runner.py:738] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=273778)\u001b[0m INFO 04-19 23:01:27 model_runner.py:738] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/data/hf/Meta-Llama-3-8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_weather(city: str):\n",
    "    \"\"\"ä¸€ä¸ªè¾“å…¥åŸå¸‚åç§°æŸ¥è¯¢å¤©æ°”çš„å‡½æ•°\n",
    "    \n",
    "    Args:\n",
    "        city: ä½ æƒ³æŸ¥è¯¢çš„åŸå¸‚åç§°\n",
    "    \"\"\"\n",
    "    if city == \"åŒ—äº¬\":\n",
    "        return \"åŒ—äº¬æ°”æ¸©36åº¦\"\n",
    "    elif city == \"å·´é»\":\n",
    "        return \"å·´é»æ°”æ¸©24åº¦\"\n",
    "    else:\n",
    "        return f\"{city}æ°”æ¸©20åº¦\"\n",
    "\n",
    "func_desc = \"\"\"function: search_weather\n",
    "  description: ä¸€ä¸ªè¾“å…¥åŸå¸‚åç§°æŸ¥è¯¢å¤©æ°”çš„å‡½æ•°\n",
    "  args:\n",
    "    city (str): ä½ æƒ³æŸ¥è¯¢çš„åŸå¸‚åç§°\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_PROMPT = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "ä½ æ˜¯ä¸€ä¸ªAIæ™ºèƒ½ï¼Œèƒ½å¤Ÿä½¿ç”¨å„ç§å‡½æ•°æ¥å›ç­”é—®é¢˜ã€‚\n",
    "\n",
    "### å¯ä»¥è®¿é—®çš„å‡½æ•°\n",
    "\n",
    "{{func_desc}}\n",
    "\n",
    "### å›å¤æ ¼å¼\n",
    "ä½¿ç”¨å¦‚ä¸‹æ ¼å¼è¿›è¡Œå›å¤ï¼š\n",
    "\n",
    "æ€è€ƒï¼š ä¸€æ­¥æ­¥æ€è€ƒè§£å†³é—®é¢˜ï¼Œå°†ä½ æ€è€ƒçš„è¿‡ç¨‹æ”¾åœ¨è¿™é‡Œ\n",
    "è¡ŒåŠ¨ï¼š\n",
    "```json\n",
    "{\n",
    "    \"function\": $FUNCTION_NAME,\n",
    "    \"args\": $FUNCTION_ARGS\n",
    "}\n",
    "```\n",
    "è§‚å¯Ÿ: å¾—åˆ°å‡½æ•°è¿”å›ç»“æœ\n",
    "...(è¿™é‡Œæ€è€ƒ/è¡ŒåŠ¨/è§‚å¯Ÿå¯ä»¥é‡å¤næ¬¡) \n",
    "æ€è€ƒï¼šç°åœ¨æˆ‘çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†\n",
    "ç­”æ¡ˆï¼šå†™å‡ºæœ€ç»ˆç­”æ¡ˆ\n",
    "\n",
    "$FUNCTION_NAME æ˜¯å‡½æ•°å. $FUNCTION_ARGS æ˜¯å¤åˆå‡½æ•°è¦æ±‚çš„å­—å…¸è¾“å…¥ã€‚<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "é—®é¢˜ï¼š{{question}} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "''' # è¿™é‡Œæœ€åä¸€å®šè¦æ·»åŠ ä¸¤ä¸ª\\n\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import Dict, List\n",
    "import re\n",
    "import json\n",
    "\n",
    "def react_agent(model, question, function, function_desc, max_rounds=3):\n",
    "    print(f\"Question: {question}\")\n",
    "    prompt = REACT_PROMPT.replace(\"{{question}}\", question).replace(\"{{func_desc}}\", function_desc)\n",
    "    \n",
    "    output = \"\"\n",
    "    try:\n",
    "        for i in range(max_rounds):\n",
    "            # Thought step\n",
    "            response = llm.generate([prompt], sampling_params=SamplingParams(temperature=0., max_tokens=1024, \n",
    "                                                                             stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],  # KEYPOINT HERE\n",
    "                                                                             stop=[\"è§‚å¯Ÿï¼š\"]))[0].outputs[0].text\n",
    "            output += response\n",
    "            prompt += response\n",
    "            print(response, end=\"\")\n",
    "            # If \"FinalAnswer\" in reponse, end react process\n",
    "            if \"ç­”æ¡ˆï¼š\" in response:\n",
    "                output += response\n",
    "                break\n",
    "\n",
    "            elif \"è¡ŒåŠ¨ï¼š\" in response:\n",
    "                args = json.loads(re.findall(\"```json([\\s\\S]*?)```\", response)[0].replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "                obs = function(**args[\"args\"])\n",
    "                obs = f\"è§‚å¯Ÿï¼š{obs}\" + \"\\næ€è€ƒï¼š\"\n",
    "                output += obs\n",
    "                prompt += obs\n",
    "                print(obs, end=\"\")\n",
    "                continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: åŒ—äº¬å’Œå·´é»ç°åœ¨å“ªä¸ªåœ°æ–¹æ›´çƒ­ï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€è€ƒï¼šé¦–å…ˆï¼Œæˆ‘éœ€è¦çŸ¥é“åŒ—äº¬å’Œå·´é»ç°åœ¨çš„å¤©æ°”æƒ…å†µã€‚\n",
      "\n",
      "è¡ŒåŠ¨ï¼š\n",
      "```json\n",
      "{\n",
      "    \"function\": \"search_weather\",\n",
      "    \"args\": {\n",
      "        \"city\": \"åŒ—äº¬\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "è§‚å¯Ÿï¼šåŒ—äº¬æ°”æ¸©36åº¦\n",
      "æ€è€ƒï¼š"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç„¶åï¼Œæˆ‘éœ€è¦çŸ¥é“å·´é»ç°åœ¨çš„å¤©æ°”æƒ…å†µã€‚\n",
      "\n",
      "è¡ŒåŠ¨ï¼š\n",
      "```json\n",
      "{\n",
      "    \"function\": \"search_weather\",\n",
      "    \"args\": {\n",
      "        \"city\": \"å·´é»\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "è§‚å¯Ÿï¼šå·´é»æ°”æ¸©24åº¦\n",
      "æ€è€ƒï¼š"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç°åœ¨æˆ‘çŸ¥é“äº†åŒ—äº¬å’Œå·´é»çš„å¤©æ°”æƒ…å†µï¼Œå¯ä»¥æ¯”è¾ƒæ°”æ¸©æ¥ç¡®å®šå“ªä¸ªåœ°æ–¹æ›´çƒ­ã€‚\n",
      "\n",
      "ç­”æ¡ˆï¼šåŒ—äº¬æ›´çƒ­ã€‚"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"åŒ—äº¬å’Œå·´é»ç°åœ¨å“ªä¸ªåœ°æ–¹æ›´çƒ­ï¼Ÿ\"\n",
    "output = react_agent(llm, question, search_weather, func_desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
