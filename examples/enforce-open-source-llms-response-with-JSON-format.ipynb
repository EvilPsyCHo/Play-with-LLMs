{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 控制LLMs输出结构\n",
    "\n",
    "由于训练数据、模型参数规模、模型能力限制，大部分开源模型比如Llama2, Mistral, Baichuan2等都无法像GPT-4一样很好的支持`function call`或者按照指定的格式输出要求返回结构。\n",
    "\n",
    "基于对语言模型预测token概率的采样规则修改，比如在需要语言模型输出数值时，屏蔽非数值token的采样概率，达到让语言模型按照指定规范生成的效果。这是在开源语言模型能力不足的过渡方法，其中一些开源方案：\n",
    "- [llamacpp grammar](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md)，灵活度中等，劣势是只支持llamacpp后端 \n",
    "- [guidance](https://github.com/guidance-ai/guidance)更加灵活，支持llamacpp/hf transformers后端，缺点是不稳定\n",
    "- [localai](https://localai.io/features/openai-functions)是对llamacpp grammar 的封装\n",
    "- [functionary](https://github.com/MeetKai/functionary)只支持很少的模型\n",
    "- [https://github.com/1rgs/jsonformer](jsonformer)\n",
    "- [lm-format-enforcer](https://github.com/noamgat/lm-format-enforcer)，活跃项目，支持vllm/llamacpp \n",
    "- [outlines]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀如何基于llamacpp grammar控制模型稳定输出Json\n",
    "\n",
    "llamacpp grammar基于一套特殊的语法，我们可以先使用`TypeScript`定义数据结构，并在[intrinsiclabs](https://grammar.intrinsiclabs.ai/)上自动转换成`grammar`语法。\n",
    "\n",
    "比如我们期望语言模型输出类似于下结构的Json:\n",
    "```json\n",
    "{\n",
    "  \"game_state\": string, # \"game over\" 或 \"game on progress\",\n",
    "  \"message\": string,\n",
    "  \"active_player\": string,\n",
    "}\n",
    "```\n",
    "\n",
    "我们先定义一个TypeScript如下，其中\n",
    "```typescript\n",
    "interface DM {\n",
    "  game_state: GameState;\n",
    "  active_player: string;\n",
    "  message: string;\n",
    "}\n",
    "\n",
    "enum GameState {\n",
    "  GameOver = \"game over\",\n",
    "  GameOnProgress = \"game on progress\",\n",
    "}\n",
    "```\n",
    "\n",
    "进入[intrinsiclabs](https://grammar.intrinsiclabs.ai/)复制粘贴就生成了`grammar`：\n",
    "\n",
    "```grammar\n",
    "root ::= DM\n",
    "GameState ::= \"\\\"game over\\\"\" | \"\\\"game on progress\\\"\"\n",
    "DM ::= \"{\"   ws   \"\\\"game_state\\\":\"   ws   GameState   \",\"   ws   \"\\\"active_player\\\":\"   ws   string   \",\"   ws   \"\\\"message\\\":\"   ws   string   \"}\"\n",
    "DMlist ::= \"[]\" | \"[\"   ws   DM   (\",\"   ws   DM)*   \"]\"\n",
    "string ::= \"\\\"\"   ([^\"]*)   \"\\\"\"\n",
    "boolean ::= \"true\" | \"false\"\n",
    "ws ::= [ \\t\\n]*\n",
    "number ::= [0-9]+   \".\"?   [0-9]*\n",
    "stringlist ::= \"[\"   ws   \"]\" | \"[\"   ws   string   (\",\"   ws   string)*   ws   \"]\"\n",
    "numberlist ::= \"[\"   ws   \"]\" | \"[\"   ws   string   (\",\"   ws   number)*   ws   \"]\"\n",
    "\n",
    "```\n",
    "\n",
    "然后我们在模型预测的时候，传入`grammar`，模型就会按照我们的要求生成Json格式的输出了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_grammar\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_grammar\n",
    "root ::= DM\n",
    "GameState ::= \"\\\"game over\\\"\" | \"\\\"game on progress\\\"\"\n",
    "DM ::= \"{\"   ws   \"\\\"game_state\\\":\"   ws   GameState   \",\"   ws   \"\\\"active_player\\\":\"   ws   string   \",\"   ws   \"\\\"message\\\":\"   ws   string   \"}\"\n",
    "DMlist ::= \"[]\" | \"[\"   ws   DM   (\",\"   ws   DM)*   \"]\"\n",
    "string ::= \"\\\"\"   ([^\"]*)   \"\\\"\"\n",
    "boolean ::= \"true\" | \"false\"\n",
    "ws ::= [ \\t\\n]*\n",
    "number ::= [0-9]+   \".\"?   [0-9]*\n",
    "stringlist ::= \"[\"   ws   \"]\" | \"[\"   ws   string   (\",\"   ws   string)*   ws   \"]\"\n",
    "numberlist ::= \"[\"   ws   \"]\" | \"[\"   ws   string   (\",\"   ws   number)*   ws   \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /data/hf/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q4_K:  833 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 24.62 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.48 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:  CUDA_Host buffer size = 25215.87 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   825.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1638\n",
      "llama_new_context_with_model: graph splits = 388\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Guessed chat format: mistral-instruct\n",
      "from_string grammar:\n",
      "root ::= DM \n",
      "DM ::= [{] ws [\"] [g] [a] [m] [e] [_] [s] [t] [a] [t] [e] [\"] [:] ws GameState [,] ws [\"] [a] [c] [t] [i] [v] [e] [_] [p] [l] [a] [y] [e] [r] [\"] [:] ws string [,] ws [\"] [m] [e] [s] [s] [a] [g] [e] [\"] [:] ws string [}] \n",
      "GameState ::= [\"] [g] [a] [m] [e] [ ] [o] [v] [e] [r] [\"] | [\"] [g] [a] [m] [e] [ ] [o] [n] [ ] [p] [r] [o] [g] [r] [e] [s] [s] [\"] \n",
      "ws ::= ws_11 \n",
      "string ::= [\"] string_8 [\"] \n",
      "DMlist ::= [[] []] | [[] ws DM DMlist_7 []] \n",
      "DMlist_6 ::= [,] ws DM \n",
      "DMlist_7 ::= DMlist_6 DMlist_7 | \n",
      "string_8 ::= string_9 \n",
      "string_9 ::= [^\"] string_9 | \n",
      "boolean ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "ws_11 ::= [ <U+0009><U+000A>] ws_11 | \n",
      "number ::= number_13 number_14 number_15 \n",
      "number_13 ::= [0-9] number_13 | [0-9] \n",
      "number_14 ::= [.] | \n",
      "number_15 ::= [0-9] number_15 | \n",
      "stringlist ::= [[] ws []] | [[] ws string stringlist_18 ws []] \n",
      "stringlist_17 ::= [,] ws string \n",
      "stringlist_18 ::= stringlist_17 stringlist_18 | \n",
      "numberlist ::= [[] ws []] | [[] ws string numberlist_21 ws []] \n",
      "numberlist_20 ::= [,] ws number \n",
      "numberlist_21 ::= numberlist_20 numberlist_21 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama, LlamaGrammar\n",
    "model = Llama(\"/data/hf/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= DM \n",
      "DM ::= [{] ws [\"] [g] [a] [m] [e] [_] [s] [t] [a] [t] [e] [\"] [:] ws GameState [,] ws [\"] [a] [c] [t] [i] [v] [e] [_] [p] [l] [a] [y] [e] [r] [\"] [:] ws string [,] ws [\"] [m] [e] [s] [s] [a] [g] [e] [\"] [:] ws string [}] \n",
      "GameState ::= [\"] [g] [a] [m] [e] [ ] [o] [v] [e] [r] [\"] | [\"] [g] [a] [m] [e] [ ] [o] [n] [ ] [p] [r] [o] [g] [r] [e] [s] [s] [\"] \n",
      "ws ::= ws_11 \n",
      "string ::= [\"] string_8 [\"] \n",
      "DMlist ::= [[] []] | [[] ws DM DMlist_7 []] \n",
      "DMlist_6 ::= [,] ws DM \n",
      "DMlist_7 ::= DMlist_6 DMlist_7 | \n",
      "string_8 ::= string_9 \n",
      "string_9 ::= [^\"] string_9 | \n",
      "boolean ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "ws_11 ::= [ <U+0009><U+000A>] ws_11 | \n",
      "number ::= number_13 number_14 number_15 \n",
      "number_13 ::= [0-9] number_13 | [0-9] \n",
      "number_14 ::= [.] | \n",
      "number_15 ::= [0-9] number_15 | \n",
      "stringlist ::= [[] ws []] | [[] ws string stringlist_18 ws []] \n",
      "stringlist_17 ::= [,] ws string \n",
      "stringlist_18 ::= stringlist_17 stringlist_18 | \n",
      "numberlist ::= [[] ws []] | [[] ws string numberlist_21 ws []] \n",
      "numberlist_20 ::= [,] ws number \n",
      "numberlist_21 ::= numberlist_20 numberlist_21 | \n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2468.43 ms\n",
      "llama_print_timings:      sample time =     284.09 ms /    47 runs   (    6.04 ms per token,   165.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     316.95 ms /     3 tokens (  105.65 ms per token,     9.47 tokens per second)\n",
      "llama_print_timings:        eval time =    6888.02 ms /    46 runs   (  149.74 ms per token,     6.68 tokens per second)\n",
      "llama_print_timings:       total time =    7624.76 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'game_state': 'game over', 'active_player': 'Player2', 'message': 'This is a happy ending after Player1 says hi and Player2 replies hello.'}\n"
     ]
    }
   ],
   "source": [
    "# completion with grammar\n",
    "grammar = LlamaGrammar.from_file(\"test_grammar\")\n",
    "prompt = \"\"\"Player1 and Player2 are playing a game named happy ending, following is the conversation between the two players:\n",
    "\n",
    "Player1: hi\n",
    "Player2: hello, how do you do recently?\n",
    "\n",
    "Now reponse the game state, action player and message with Json format. Message involves the environment description string.\"\"\"\n",
    "res = model.create_completion(prompt, max_tokens=1000, grammar=grammar)\n",
    "print(json.loads(res[\"choices\"][0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2468.43 ms\n",
      "llama_print_timings:      sample time =     106.04 ms /   291 runs   (    0.36 ms per token,  2744.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   44112.52 ms /   291 runs   (  151.59 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:       total time =   44874.16 ms /   292 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gameState': 'ongoing', 'actionPlayer': 'Player2', 'message': {'conversation': {'current': [{'player': 'Player1', 'statement': 'hi'}, {'player': 'Player2', 'statement': 'hello, how do you do recently?'}]}, 'environment': {'players': [{'id': 'Player1', 'name': 'Player1', 'status': 'active', 'score': 0}, {'id': 'Player2', 'name': 'Player2', 'status': 'active', 'score': 0}], 'gameName': 'happy ending'}}}\n"
     ]
    }
   ],
   "source": [
    "# completion without grammar\n",
    "res = model.create_completion(prompt, max_tokens=1000)\n",
    "print(json.loads(res[\"choices\"][0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the regular completion, using grammar for prediction has excellent consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 😀 A more preferred way for Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77046/3400868137.py:13: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
      "  grammer = LlamaGrammar.from_json_schema(DM.schema_json())\n",
      "from_string grammar:\n",
      "space ::= space_1 \n",
      "space_1 ::= [ ] | \n",
      "game-state-GameState ::= [\"] [g] [a] [m] [e] [ ] [o] [v] [e] [r] [\"] | [\"] [g] [a] [m] [e] [ ] [o] [n] [ ] [p] [r] [o] [g] [r] [e] [s] [s] [\"] \n",
      "string ::= [\"] string_6 [\"] space \n",
      "string_4 ::= [^\"\\] | [\\] string_5 \n",
      "string_5 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_6 ::= string_4 string_6 | \n",
      "root ::= [{] space [\"] [g] [a] [m] [e] [_] [s] [t] [a] [t] [e] [\"] space [:] space game-state-GameState [,] space [\"] [m] [e] [s] [s] [a] [g] [e] [\"] space [:] space string [,] space [\"] [a] [c] [t] [i] [v] [e] [_] [p] [l] [a] [y] [e] [r] [\"] space [:] space string [}] space \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class GameState(str, Enum):\n",
    "    game_over = \"game over\"\n",
    "    game_active = \"game on progress\"\n",
    "\n",
    "class DM(BaseModel):\n",
    "    game_state : GameState\n",
    "    message: str\n",
    "    active_player: str\n",
    "\n",
    "grammer = LlamaGrammar.from_json_schema(DM.schema_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2468.43 ms\n",
      "llama_print_timings:      sample time =     257.82 ms /    43 runs   (    6.00 ms per token,   166.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    6492.99 ms /    43 runs   (  151.00 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:       total time =    6873.88 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'game_state': 'game on progress', 'active_player': 'Player1', 'message': \"Player1 said 'hi', Player2 replied saying 'hello'\"}\n"
     ]
    }
   ],
   "source": [
    "res = model.create_completion(prompt, max_tokens=1000, grammar=grammar)\n",
    "print(json.loads(res[\"choices\"][0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pydantic` works great!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
