{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€Mixtral-8x7B-instructå®ç°ReAct Agentå¹¶æ”¯æŒå·¥å…·è°ƒç”¨\n",
    "\n",
    "![](../assets/simple-react.gif)\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œå¦‚æœä½ è¿˜ä¸äº†è§£å¦‚ä½•åˆ©ç”¨grammarè®©LLMsæŒ‰ç…§ä½ è¦æ±‚çš„æ ¼å¼è¾“å‡ºï¼Œç‚¹å‡»[mistral-stable-output-JSON-format](./mistral-stable-output-JSON-format.ipynb)æŸ¥çœ‹å¦‚ä½•æ§åˆ¶æ¨¡å‹è¾“å‡ºæ ¼å¼ã€‚\n",
    "å‚è€ƒï¼š\n",
    "- https://github.com/Maximilian-Winter/llama-cpp-agent\n",
    "- https://www.reddit.com/r/MistralAI/comments/191wg3i/react_prompting_mistral_or_mixtral/\n",
    "- https://pastebin.com/a94xp56b\n",
    "- https://pastebin.com/H3a9HiHw\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*adwhd8hmwcCAYTZNMiSAxA.png)\n",
    "\n",
    "CoTè®©æ¨¡å‹å…·å¤‡äº†é—®é¢˜åˆ†è§£ä¸æ¨ç†ï¼ˆï¼Ÿï¼‰èƒ½åŠ›ï¼Œä½†å—é™äºæ¨¡å‹çŸ¥è¯†å®ƒä¸çŸ¥é“ä»Šå¤©åŒ—äº¬çš„å¤©æ°”ï¼Œä¹Ÿä¸çŸ¥é“æœ€æ–°çš„æ–°é—»èµ„è®¯ï¼Œä¹Ÿæ— æ³•è¿›è¡Œå¤æ‚çš„æ•°å­¦å…ƒç®—ã€‚åŒæ—¶ï¼Œä¸€äº›å¤æ‚é—®é¢˜å¾€å¾€æ— æ³•é€šè¿‡å•è½®çš„äº¤äº’å®Œæˆï¼Œæ¨¡å‹å¾€å¾€éœ€è¦è¿›è¡Œå¤šè½®çš„ä¿¡æ¯æ”¶é›†ã€è®¡ç®—ã€å·¥å…·è°ƒç”¨æ‰èƒ½å¾—åˆ°åˆç†ç­”æ¡ˆã€‚ReActï¼ˆReasoning and Actingï¼‰æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶æ¨¡å‹äº¤äº’è¿‡ç¨‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "```text\n",
    "Question: ç”¨æˆ·é—®é¢˜\n",
    "Assistant:\n",
    "  Thought: æ€è€ƒ\n",
    "  Action: è°ƒç”¨tool/function\n",
    "  Observation: åé¦ˆæ‰§è¡Œç»“æœ\n",
    "  ...å¾ªç¯Næ¬¡\n",
    "  Thought: Now, I know the final answer for this question.\n",
    "  Final Answer: æœ€ç»ˆç­”æ¡ˆ\n",
    "```\n",
    "\n",
    "ä¸€èˆ¬æˆ‘ä»¬éœ€è¦åœ¨æ¨¡å‹è¾“å‡º`Observation: `æ—¶æ‰“æ–­æ¨¡å‹ï¼Œè°ƒç”¨å‡½æ•°æˆ–å·¥å…·APIæ‰§è¡Œï¼Œå¹¶å°†ç»“æœæ‹¼æ¥åˆ°`Observation: `åï¼Œå¦åˆ™æ¨¡å‹ä¼šè‡ªå·±æƒ³è±¡ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸ºäº†ç¡®ä¿æ¨¡å‹äº†è§£æ‰€æœ‰å¯è°ƒç”¨çš„å‡½æ•°æˆ–å·¥å…·ï¼Œåœ¨è¾“å…¥æç¤ºè¯ï¼ˆsystem æˆ– userï¼‰ä¸­éœ€è¦éœ€è¦å‘ŠçŸ¥æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ¯”å¦‚ï¼š\n",
    "\n",
    "```text\n",
    "You are an AI agent capable of using a variety of functions to answer a question.\n",
    "\n",
    "{{func_desc}}\n",
    "\n",
    "Response using the follow format:\n",
    "\n",
    "Thought: think step by steps, how to solve the question, put your thought process here.\n",
    "Action: {\n",
    "    \"function\": $FUNCTION_NAME,\n",
    "    \"args\": $FUNCTION_ARGS,\n",
    "}\n",
    "Observation: function output\n",
    "...(this Thought/Action/Observation can repeat N times) \n",
    "Thought: Now I know the final answer.\n",
    "Final Answer: the final answer\n",
    "\n",
    "$FUNCTION_NAME is the name of function. $FUNCTION_ARGS is a dictionary input matching the requirement of function.\n",
    "\n",
    "<START>\n",
    "Question: {{question}}\\n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama, LlamaGrammar\n",
    "model = Llama(\"/data/hf/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_weather(city: str):\n",
    "    \"\"\"A useful function to check the weather.\n",
    "    \n",
    "    Args:\n",
    "        city: the city where you want to check the weather\n",
    "    \"\"\"\n",
    "    if city == \"åŒ—äº¬\":\n",
    "        return \"åŒ—äº¬æ°”æ¸©36åº¦\"\n",
    "    elif city == \"å·´é»\":\n",
    "        return \"å·´é»æ°”æ¸©24åº¦\"\n",
    "    else:\n",
    "        return f\"{city}æ°”æ¸©20åº¦\"\n",
    "\n",
    "func_desc = \"\"\"function: search_weather\n",
    "  description: A useful function to check the weather.\n",
    "  args:\n",
    "    city (str): the city where you want to check the weather\n",
    "\"\"\"\n",
    "\n",
    "# from typing import List, Optional, Union, Dict\n",
    "# from llama_cpp_agent.gbnf_grammar_generator.gbnf_grammar_from_pydantic_models import generate_text_documentation, create_dynamic_model_from_function, generate_gbnf_grammar\n",
    "# func_desc = generate_text_documentation([create_dynamic_model_from_function(search_weather)], model_prefix=\"function\", fields_prefix=\"args\")\n",
    "# func_grammar = generate_gbnf_grammar(create_dynamic_model_from_function(search_weather), set(), dict())\n",
    "# print(func_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_PROMPT = '''[INST] You are an AI agent capable of using a variety of functions to answer a question.\n",
    "\n",
    "### Accessable Function\n",
    "\n",
    "{{func_desc}}\n",
    "\n",
    "### Reponse Format\n",
    "Response using the follow format:\n",
    "\n",
    "Thought: think step by steps, how to solve the question, put your thought process here.\n",
    "Action:\n",
    "```json\n",
    "{\n",
    "    \"function\": $FUNCTION_NAME,\n",
    "    \"args\": $FUNCTION_ARGS\n",
    "}\n",
    "```\n",
    "Observation: function output\n",
    "...(this Thought/Action/Observation can repeat N times) \n",
    "Thought: I now know the final answer \n",
    "Final Answer: the final answer\n",
    "\n",
    "$FUNCTION_NAME is the name of function. $FUNCTION_ARGS is a dictionary input matching the requirement of function.\n",
    "\n",
    "<START>\n",
    "Question: {{question}} [\\INST]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import Dict, List\n",
    "import re\n",
    "import json\n",
    "\n",
    "def react_agent(model, question, function, function_desc, max_rounds=3):\n",
    "    # construct input prompt\n",
    "    print(f\"Question: {question}\")\n",
    "    prompt = REACT_PROMPT.replace(\"{{question}}\", question).replace(\"{{func_desc}}\", function_desc)\n",
    "    output = \"\"\n",
    "    try:\n",
    "        for i in range(max_rounds):\n",
    "            # Thought step\n",
    "            response = model.create_completion(prompt, stop=[\"Observation: \"], max_tokens=400)[\"choices\"][0][\"text\"]\n",
    "            output += response\n",
    "            print(response, end=\"\")\n",
    "            # If \"FinalAnswer\" in reponse, end react process\n",
    "            if \"Final Anwser:\" in response:\n",
    "                output += response\n",
    "                break\n",
    "\n",
    "            elif \"Action:\" in response:\n",
    "                args = json.loads(re.findall(\"```json([\\s\\S]*?)```\", response)[0].replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "                obs = function(**args[\"args\"])\n",
    "                obs = f\"Observation: {obs}\" + \"\\nThought: \"\n",
    "                output += obs\n",
    "                prompt += obs\n",
    "                print(obs, end=\"\")\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: åŒ—äº¬å’Œå·´é»ç°åœ¨å“ªä¸ªåœ°æ–¹æ›´çƒ­ï¼Ÿ\n",
      "Thought: The question asks for a comparison of the current temperature in Beijing and Paris. I can use the search\\_weather function to find out. I will need to call the function twice, once for each city.\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "    \"function\": \"search_weather\",\n",
      "    \"args\": {\n",
      "        \"city\": \"åŒ—äº¬\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "Observation: åŒ—äº¬æ°”æ¸©36åº¦\n",
      "Thought:  To answer this question, I need to check the current temperature in Beijing and Paris. I already know the current temperature in Beijing is 36 degrees. Now, I should get the temperature in Paris.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "    \"function\": \"search_weather\",\n",
      "    \"args\": {\n",
      "        \"city\": \"Paris\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "Observation: Parisæ°”æ¸©20åº¦\n",
      "Thought: åŒ—äº¬çš„æ¸©åº¦æ¯”å·´é»é«˜, so I can answer the question now.\n",
      "Final Answer: ç°åœ¨åŒ—äº¬æ›´çƒ­ã€‚"
     ]
    }
   ],
   "source": [
    "question = \"åŒ—äº¬å’Œå·´é»ç°åœ¨å“ªä¸ªåœ°æ–¹æ›´çƒ­ï¼Ÿ\"\n",
    "output = react_agent(model, question, search_weather, func_desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
