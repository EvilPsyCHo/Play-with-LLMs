{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Llama3-8b-Instruct React Agent Advanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/gomars/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-02 15:57:50,393\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 15:57:51 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:57:53,935\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 15:57:54 llm_engine.py:79] Initializing an LLM engine with config: model='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer='/data/hf/Meta-Llama-3-8B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-02 15:58:06 llm_engine.py:337] # GPU blocks: 12465, # CPU blocks: 4096\n",
      "INFO 05-02 15:58:07 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-02 15:58:07 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=1490933)\u001b[0m INFO 05-02 15:58:07 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=1490933)\u001b[0m INFO 05-02 15:58:07 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=1490933)\u001b[0m INFO 05-02 15:58:16 model_runner.py:738] Graph capturing finished in 9 secs.\n",
      "INFO 05-02 15:58:16 model_runner.py:738] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from llama_index.core.agent import ReActAgent\n",
    "llm = LLM(\n",
    "    model=\"/data/hf/Meta-Llama-3-8B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful Assistant named eric simon<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"You are a helpful Assistant named eric simon\"}, {\"role\": \"user\", \"content\": \"who are you?\"}], \n",
    "                              tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! I'm Eric Simon, a helpful Assistant here to assist you with any questions or tasks you may have. I'm a large language model, trained on a vast amount of text data, which enables me to understand and respond to a wide range of topics and inquiries. My primary goal is to provide accurate and helpful information, answer your questions, and even engage in conversation to make your experience more enjoyable and productive. So, feel free to ask me anything, and I'll do my best to help you out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_params = SamplingParams(max_tokens=512, temperature=0., stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], stop=\"Eric Simon Jim\")\n",
    "model_output = llm.generate([prompt], sampling_params=sampling_params)\n",
    "print(model_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Tool Name: search_weather\n",
      "Tool Description: search_weather(city_name: str) -> str\n",
      "check real-time weather and temperature of a city\n",
      "Tool Args: {\"type\": \"object\", \"properties\": {\"city_name\": {\"title\": \"City Name\", \"type\": \"string\"}}, \"required\": [\"city_name\"]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from typing import Dict, List, Any, Sequence, Callable\n",
    "import re\n",
    "import json\n",
    "from rich import print\n",
    "\n",
    "\n",
    "REACT_PROMPT = '''You are an AI agent capable of using a variety of tools to answer question.\n",
    "\n",
    "You have access to the following tools:\n",
    "{{tools_desc}}\n",
    "\n",
    "### Reponse Format\n",
    "\n",
    "Response using the follow format:\n",
    "\n",
    "Thought: think step by steps, how to solve the question, put your thought process here.\n",
    "Action:\n",
    "```json\n",
    "{\n",
    "    \"tool\": $TOOL_NAME,\n",
    "    \"args\": $TOOL_ARGS\n",
    "}\n",
    "```\n",
    "Observation: tool output\n",
    "...(this Thought/Action/Observation can repeat N times until you get enough information to answer the question)\n",
    "Thought: I now know the final answer \n",
    "Final Answer: make sure output the final answer here\n",
    "\n",
    "$TOOL_NAME is the name of the tool. $TOOL_ARGS is a dictionary input matching the requirement of the tool.\n",
    "'''\n",
    "\n",
    "\n",
    "# copy from llama-index\n",
    "# get function tools description\n",
    "def get_react_tool_descriptions(tools: Sequence[BaseTool]) -> List[str]:\n",
    "    \"\"\"Tool.\"\"\"\n",
    "    tool_descs = []\n",
    "    for tool in tools:\n",
    "        tool_desc = (\n",
    "            f\"> Tool Name: {tool.metadata.name}\\n\"\n",
    "            f\"Tool Description: {tool.metadata.description}\\n\"\n",
    "            f\"Tool Args: {tool.metadata.fn_schema_str}\\n\"\n",
    "        )\n",
    "        tool_descs.append(tool_desc)\n",
    "    return tool_descs\n",
    "\n",
    "\n",
    "def search_weather(city_name: str) -> str:\n",
    "    \"\"\"check real-time weather and temperature of a city\"\"\"\n",
    "    return \"temperature 27, weather rain\"\n",
    "\n",
    "\n",
    "print(get_react_tool_descriptions([FunctionTool.from_defaults(search_weather)])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActOutputParser:\n",
    "    \"\"\"ReAct Output parser.\"\"\"\n",
    "\n",
    "    def parse(self, output: str):\n",
    "        if \"Final Answer:\" in output:\n",
    "            return self.extract_final_answer(output)\n",
    "\n",
    "        if \"Action:\" in output:\n",
    "            return self.extract_action(output)\n",
    "\n",
    "        raise ValueError(f\"Could not parse output: {output}\")\n",
    "    \n",
    "    def extract_action(self, input_text: str):\n",
    "        pattern = r\"Thought:([\\s\\S]*?)Action:([\\s\\S]*)\"\n",
    "        match = re.search(pattern, input_text)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not extract Thought/Action from input text: {input_text}\")\n",
    "\n",
    "        thought = match.group(1).strip()\n",
    "        action = match.group(2).strip()\n",
    "\n",
    "        json_block_pattern = \"```json([\\s\\S]*?)(```|$)\"\n",
    "        match = re.search(json_block_pattern, action)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not extract Action JSON block from input text: {action}\")\n",
    "        action_json = match.group(1).strip()\n",
    "        import dirtyjson\n",
    "        try:\n",
    "            action_json = dirtyjson.loads(action_json)\n",
    "        except:\n",
    "            raise ValueError(f\"Unable parse JSON from {action_json}\")\n",
    "        \n",
    "        import json\n",
    "        raw_message = json.dumps(action_json, indent=4)\n",
    "        return {\n",
    "            \"step\": \"reasoning_acting\", \n",
    "            \"content\": {\"thought\": thought, \"action\": action_json}, \n",
    "            \"raw_message\": input_text}\n",
    "\n",
    "\n",
    "    def extract_final_answer(self, response):\n",
    "        pattern = r\"\\s*Thought:([\\s\\S]*?)Final Answer:([\\s\\S]*?)(?:$)\"\n",
    "        match = re.search(pattern, response, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\n",
    "                f\"Could not extract final answer from input text: {response}\"\n",
    "            )\n",
    "\n",
    "        thought = match.group(1).strip()\n",
    "        answer = match.group(2).strip()\n",
    "        raw_message = f\"Thought: {thought}\\nFinal Answer: {answer}\"\n",
    "        return {\n",
    "            \"step\": \"answer\", \n",
    "            \"content\": {\"thought\": thought, \"answer\": answer}, \n",
    "            \"raw_message\": response,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActAgent:\n",
    "\n",
    "    def __init__(self, llm, tools: List[Callable], system_prompt: str=None, max_rounds: int=10, output_parser=None):\n",
    "        self.llm = llm\n",
    "        self.tokenizer = llm.get_tokenizer()\n",
    "        tools = [FunctionTool.from_defaults(fn=t) for t in tools]\n",
    "        self.tools = {f.metadata.get_name(): f for f in tools}\n",
    "        self.system_prompt = system_prompt or REACT_PROMPT\n",
    "        self.max_rounds = max_rounds\n",
    "        self.output_parser = output_parser or ReActOutputParser()\n",
    "        self.is_llama3 = \"<|eot_id|>\" in tokenizer.vocab\n",
    "    \n",
    "    def format_system_prompt(self):\n",
    "        tools_name = \", \".join([tool.metadata.get_name() for tool in self.tools.values()])\n",
    "        tools_desc = \"\\n\".join(get_react_tool_descriptions(self.tools.values()))\n",
    "        return self.system_prompt.replace(\"{{tools_desc}}\", tools_desc).replace(\"{{tools_name}}\", tools_name)\n",
    "\n",
    "    def generate(self, messages, stop=None, max_tokens=1024):\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")] if self.is_llama3 else [tokenizer.eos_token_id]\n",
    "        sampling_params = SamplingParams(max_tokens=max_tokens, stop=stop, stop_token_ids=stop_token_ids, temperature=0.)\n",
    "        model_output = llm.generate([prompt], sampling_params=sampling_params)[0].outputs[0].text\n",
    "        return model_output\n",
    "        \n",
    "    def run(self, query, num_rounds=None, verbose=False):\n",
    "        # Initialization\n",
    "        num_rounds = min(num_rounds, self.max_rounds) if num_rounds else self.max_rounds\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.format_system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "            ]\n",
    "        trajectory = [{\"step\": \"query\", \"content\": query, \"raw_message\": query}]\n",
    "\n",
    "        # query\n",
    "        if verbose:\n",
    "            print(f\"[green3]Query[/green3]\")\n",
    "            print(query)\n",
    "        for _ in range(1, 1+num_rounds):\n",
    "            model_output = self.generate(messages, stop=\"Observation:\")\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_output})\n",
    "            try:\n",
    "                step_state = self.output_parser.parse(model_output)\n",
    "            except Exception as e:\n",
    "                return {\"response\": str(e), \"trajectory\": trajectory, \"successful\": False}\n",
    "            if verbose:\n",
    "                print(f\"[green3]{step_state['step']}[/green3]\")\n",
    "                print(step_state[\"raw_message\"])\n",
    "            trajectory.append(step_state)\n",
    "            if step_state[\"step\"] == \"reasoning_acting\":\n",
    "                func = self.tools[step_state[\"content\"][\"action\"][\"tool\"]]\n",
    "                args = step_state[\"content\"][\"action\"][\"args\"]\n",
    "                try:\n",
    "                    observation = func(**args).content\n",
    "                except Exception as e:\n",
    "                    return {\"response\": str(e), \"trajectory\": trajectory, \"successful\": False}\n",
    "                if verbose:\n",
    "                    print(f\"[green3]observation[/green3]\")\n",
    "                    print(observation)\n",
    "                trajectory.append({\"step\": \"observation\", \"content\": observation, \"raw_message\": observation})\n",
    "                messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
    "            elif step_state[\"step\"] == \"answer\":\n",
    "                return {\"response\": step_state[\"content\"][\"answer\"], \"trajectory\": trajectory, \"successful\": True}\n",
    "        return {\"response\": \"Maximum number of iterations exceeded\", \"trajectory\": trajectory, \"successful\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two integers and returns the result integer\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divides two integers and returns the result integer\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [multiply, add, subtract, divide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">Query</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40mQuery\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">121</span> + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> * <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> *<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is \u001b[1m(\u001b[0m\u001b[1;36m121\u001b[0m + \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m * \u001b[1;36m5\u001b[0m *\u001b[1;36m100\u001b[0m?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">reasoning_acting</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40mreasoning_acting\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Thought: First, I need to calculate the expression inside the parentheses, which is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">121</span> + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Then, I need to \n",
       "multiply the result by <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, and finally, I need to multiply the result by <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>.\n",
       "\n",
       "Action:\n",
       "```json\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tool\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"add\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"args\"</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"a\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">121</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"b\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "```\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Thought: First, I need to calculate the expression inside the parentheses, which is \u001b[1;36m121\u001b[0m + \u001b[1;36m2\u001b[0m. Then, I need to \n",
       "multiply the result by \u001b[1;36m5\u001b[0m, and finally, I need to multiply the result by \u001b[1;36m100\u001b[0m.\n",
       "\n",
       "Action:\n",
       "```json\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"tool\"\u001b[0m: \u001b[32m\"add\"\u001b[0m,\n",
       "    \u001b[32m\"args\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m\"a\"\u001b[0m: \u001b[1;36m121\u001b[0m, \u001b[32m\"b\"\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "```\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">observation</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40mobservation\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">123</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m123\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">reasoning_acting</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40mreasoning_acting\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Thought: Now that I have the result of the addition, I need to multiply it by <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.\n",
       "\n",
       "Action:\n",
       "```json\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tool\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"multiply\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"args\"</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"a\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">123</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"b\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "```\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Thought: Now that I have the result of the addition, I need to multiply it by \u001b[1;36m5\u001b[0m.\n",
       "\n",
       "Action:\n",
       "```json\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"tool\"\u001b[0m: \u001b[32m\"multiply\"\u001b[0m,\n",
       "    \u001b[32m\"args\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m\"a\"\u001b[0m: \u001b[1;36m123\u001b[0m, \u001b[32m\"b\"\u001b[0m: \u001b[1;36m5\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "```\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">observation</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40mobservation\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">615</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m615\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">reasoning_acting</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40mreasoning_acting\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Thought: Now that I have the result of the multiplication, I need to multiply it by <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>.\n",
       "\n",
       "Action:\n",
       "```json\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tool\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"multiply\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"args\"</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"a\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">615</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"b\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Thought: Now that I have the result of the multiplication, I need to multiply it by \u001b[1;36m100\u001b[0m.\n",
       "\n",
       "Action:\n",
       "```json\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"tool\"\u001b[0m: \u001b[32m\"multiply\"\u001b[0m,\n",
       "    \u001b[32m\"args\"\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m\"a\"\u001b[0m: \u001b[1;36m615\u001b[0m, \u001b[32m\"b\"\u001b[0m: \u001b[1;36m100\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">observation</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40mobservation\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61500</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m61500\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d700; text-decoration-color: #00d700\">answer</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;40manswer\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Thought: I have now calculated the final result, which is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61500</span>.\n",
       "\n",
       "Final Answer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61500</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Thought: I have now calculated the final result, which is \u001b[1;36m61500\u001b[0m.\n",
       "\n",
       "Final Answer: \u001b[1;36m61500\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'response': '61500',\n",
       " 'trajectory': [{'step': 'query',\n",
       "   'content': 'What is (121 + 2) * 5 *100?',\n",
       "   'raw_message': 'What is (121 + 2) * 5 *100?'},\n",
       "  {'step': 'reasoning_acting',\n",
       "   'content': {'thought': 'First, I need to calculate the expression inside the parentheses, which is 121 + 2. Then, I need to multiply the result by 5, and finally, I need to multiply the result by 100.',\n",
       "    'action': AttributedDict([('tool', 'add'), ('args', AttributedDict([('a', 121), ('b', 2)]))])},\n",
       "   'raw_message': 'Thought: First, I need to calculate the expression inside the parentheses, which is 121 + 2. Then, I need to multiply the result by 5, and finally, I need to multiply the result by 100.\\n\\nAction:\\n```json\\n{\\n    \"tool\": \"add\",\\n    \"args\": {\"a\": 121, \"b\": 2}\\n}\\n```\\n'},\n",
       "  {'step': 'observation', 'content': '123', 'raw_message': '123'},\n",
       "  {'step': 'reasoning_acting',\n",
       "   'content': {'thought': 'Now that I have the result of the addition, I need to multiply it by 5.',\n",
       "    'action': AttributedDict([('tool', 'multiply'), ('args', AttributedDict([('a', 123), ('b', 5)]))])},\n",
       "   'raw_message': 'Thought: Now that I have the result of the addition, I need to multiply it by 5.\\n\\nAction:\\n```json\\n{\\n    \"tool\": \"multiply\",\\n    \"args\": {\"a\": 123, \"b\": 5}\\n}\\n```\\n\\n'},\n",
       "  {'step': 'observation', 'content': '615', 'raw_message': '615'},\n",
       "  {'step': 'reasoning_acting',\n",
       "   'content': {'thought': 'Now that I have the result of the multiplication, I need to multiply it by 100.',\n",
       "    'action': AttributedDict([('tool', 'multiply'), ('args', AttributedDict([('a', 615), ('b', 100)]))])},\n",
       "   'raw_message': 'Thought: Now that I have the result of the multiplication, I need to multiply it by 100.\\n\\nAction:\\n```json\\n{\\n    \"tool\": \"multiply\",\\n    \"args\": {\"a\": 615, \"b\": 100}\\n}\\n```'},\n",
       "  {'step': 'observation', 'content': '61500', 'raw_message': '61500'},\n",
       "  {'step': 'answer',\n",
       "   'content': {'thought': 'I have now calculated the final result, which is 61500.',\n",
       "    'answer': '61500'},\n",
       "   'raw_message': 'Thought: I have now calculated the final result, which is 61500.\\n\\nFinal Answer: 61500'}],\n",
       " 'successful': True}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ReActAgent(llm, tools)\n",
    "agent.run(\"What is (121 + 2) * 5 *100?\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
