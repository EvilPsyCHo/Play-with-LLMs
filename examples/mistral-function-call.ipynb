{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_Ws6klOY0lGwN8tuVMEPCOpT7', function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_drZUDa3Q318AbiWMzk8uDVm2', function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_ByCskPWPRfZccDZKgyeMLl5E', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=\"sk-DuN7MOXUXb4NIGCN300a03E7129947718979198cEe1385D7\", base_url=\"https://api.xi-ai.cn/v1\")\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    return tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response\n",
    "tool_call = run_conversation()\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Function(arguments=\\'{\"location\": \"San Francisco\", \"unit\": \"celsius\"}\\', name=\\'get_current_weather\\')'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(tool_call[0].function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://github.com/Maximilian-Winter/llama-cpp-agent/blob/master/src/llama_cpp_agent/function_calling.py\n",
    "# https://github.com/Maximilian-Winter/llama-cpp-agent/blob/1d5ade2a50e542522703c62adb796d6f50da7262/src/llama_cpp_agent/function_calling_agent.py#L237\n",
    "# https://github.com/Maximilian-Winter/llama-cpp-agent/blob/1d5ade2a50e542522703c62adb796d6f50da7262/src/llama_cpp_agent/function_calling.py#L202\n",
    "# https://github.com/Maximilian-Winter/llama-cpp-agent/blob/1d5ade2a50e542522703c62adb796d6f50da7262/src/llama_cpp_agent/gbnf_grammar_generator/gbnf_grammar_from_pydantic_models.py#L655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function(arguments='{\"location\": \"San Francisco\"}', name='get_current_weather')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call[0].function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import BaseTool, FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = multiply_tool.to_langchain_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply(a: int, b: int) -> int\\nMultiple two integers and returns the result integer'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiply_output(__root__=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.output_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_documentation(\n",
    "    pydantic_models: list[type[BaseModel]],\n",
    "    model_prefix=\"Model\",\n",
    "    fields_prefix=\"Fields\",\n",
    "    documentation_with_field_description=True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate markdown documentation for a list of Pydantic models.\n",
    "\n",
    "    Args:\n",
    "        pydantic_models (list[type[BaseModel]]): list of Pydantic model classes.\n",
    "        model_prefix (str): Prefix for the model section.\n",
    "        fields_prefix (str): Prefix for the fields section.\n",
    "        documentation_with_field_description (bool): Include field descriptions in the documentation.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text documentation.\n",
    "    \"\"\"\n",
    "    documentation = \"\"\n",
    "    pyd_models = [(model, True) for model in pydantic_models]\n",
    "    for model, add_prefix in pyd_models:\n",
    "        if add_prefix:\n",
    "            documentation += f\"{model_prefix}: {model.__name__}\\n\"\n",
    "        else:\n",
    "            documentation += f\"model: {model.__name__}\\n\"\n",
    "\n",
    "        # Handling multi-line model description with proper indentation\n",
    "\n",
    "        class_doc = getdoc(model)\n",
    "        base_class_doc = getdoc(BaseModel)\n",
    "        class_description = (\n",
    "            class_doc if class_doc and class_doc != base_class_doc else \"\"\n",
    "        )\n",
    "        if class_description != \"\":\n",
    "            documentation += \"  description: \"\n",
    "            documentation += format_multiline_description(class_description, 2) + \"\\n\"\n",
    "\n",
    "        if add_prefix:\n",
    "            # Indenting the fields section\n",
    "            documentation += f\"  {fields_prefix}:\\n\"\n",
    "        else:\n",
    "            documentation += f\"  fields:\\n\"\n",
    "        if isclass(model) and issubclass(model, BaseModel):\n",
    "            for name, field_type in model.__annotations__.items():\n",
    "                # if name == \"markdown_code_block\":\n",
    "                #    continue\n",
    "                if get_origin(field_type) == list:\n",
    "                    element_type = get_args(field_type)[0]\n",
    "                    if isclass(element_type) and issubclass(element_type, BaseModel):\n",
    "                        pyd_models.append((element_type, False))\n",
    "                if get_origin(field_type) == Union:\n",
    "                    element_types = get_args(field_type)\n",
    "                    for element_type in element_types:\n",
    "                        if isclass(element_type) and issubclass(\n",
    "                            element_type, BaseModel\n",
    "                        ):\n",
    "                            pyd_models.append((element_type, False))\n",
    "                if isclass(field_type) and issubclass(field_type, BaseModel):\n",
    "                    pyd_models.append((field_type, False))\n",
    "                documentation += generate_field_text(\n",
    "                    name,\n",
    "                    field_type,\n",
    "                    model,\n",
    "                    documentation_with_field_description=documentation_with_field_description,\n",
    "                )\n",
    "            if add_prefix:\n",
    "                if documentation.endswith(f\"{fields_prefix}:\\n\"):\n",
    "                    documentation += \"    none\\n\"\n",
    "            else:\n",
    "                if documentation.endswith(\"fields:\\n\"):\n",
    "                    documentation += \"    none\\n\"\n",
    "            documentation += \"\\n\"\n",
    "\n",
    "        if (\n",
    "            hasattr(model, \"Config\")\n",
    "            and hasattr(model.Config, \"json_schema_extra\")\n",
    "            and \"example\" in model.Config.json_schema_extra\n",
    "        ):\n",
    "            documentation += f\"  Expected Example Output for {format_model_and_field_name(model.__name__)}:\\n\"\n",
    "            json_example = json.dumps(model.Config.json_schema_extra[\"example\"])\n",
    "            documentation += format_multiline_description(json_example, 2) + \"\\n\"\n",
    "\n",
    "    return documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
