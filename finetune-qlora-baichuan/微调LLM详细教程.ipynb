{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e489af-bcea-45d2-ad94-a7a524ed2d76",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 微调LLM详细教程\n",
    "\n",
    "![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\n",
    "\n",
    "1. 数据\n",
    "\n",
    "- 单轮对话场景\n",
    "- 多轮对话场景\n",
    "- labels mask\n",
    "\n",
    "2. 模型\n",
    "\n",
    "- 量化\n",
    "- how to find lora modules / lora config (p-tuning等方法之后有机会出)\n",
    "- Loss and input_ids shift\n",
    "\n",
    "3. 训练\n",
    "\n",
    "- Trainer, TrainingArguments, HfArgumentParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b7a90-c425-4bf7-82bc-d9e538c9b920",
   "metadata": {},
   "source": [
    "# 数据\n",
    "\n",
    "选取`BelleGroup/train_0.5M_CN`数据集来对语言模型训练数据集处理进行介绍."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8f8ae9d-c76b-41a9-9f9c-ff854678591d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/kky/.cache/huggingface/datasets/json/default-4d98de2320813175/0.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': '给定一个英文句子，翻译成中文。\\nI love to learn new things every day.\\n',\n",
       " 'input': '',\n",
       " 'output': '我每天喜欢学习新事物。'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "# https://huggingface.co/datasets/BelleGroup/train_0.5M_CN/blob/main/Belle_open_source_0.5M.json\n",
    "ds = Dataset.from_json(\"./data/Belle_open_source_0.5M.json\")\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13121e6f-6748-4e56-a692-98cc0a1ea997",
   "metadata": {},
   "source": [
    "可以观察到数据是单轮对话数据,输入`instruction`和`input`,希望模型能够按照`output`进行回复. Decoder-Only语言模型通过预测下一个token进行训练,因此我们需要按照需要的场景对数据进行组织."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5930399-a9b3-4ce9-b6c2-b180cf50f88b",
   "metadata": {},
   "source": [
    "![](http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c08b3-4742-4f35-b20b-e8b9f3a58447",
   "metadata": {},
   "source": [
    "## 单轮对话场景\n",
    "\n",
    "**方案(1)**:直接将`instruction`, `input`, `output` 合并,作为文本输入,模型每次预测下一个单词. 这种方法简单,但是存在2个问题.\n",
    "1. 模型无法学习到何时结束对话(输出`eos_token`即`end of sentence toeken`)\n",
    "2. 模型无法对问题和回答做有效的区分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b553f857-46cf-4078-a880-ca503050d168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给定一个英文句子，翻译成中文。\n",
      "I love to learn new things every day.\n",
      "我每天喜欢学习新事物。\n"
     ]
    }
   ],
   "source": [
    "print(ds[0]['instruction'] + ds[0]['input'] + ds[0]['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc3e6d-6231-41bb-b43c-2ae94c8a9c1b",
   "metadata": {},
   "source": [
    "**方案(2)**:在方案(1)基础上,最后一个位置添加`eos_token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84300d75-fd20-43c1-9363-3ca04a0aa7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给定一个英文句子，翻译成中文。\n",
      "I love to learn new things every day.\n",
      "我每天喜欢学习新事物。</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./pretrained/baichuan-7b/\", trust_remote_code=True)\n",
    "\n",
    "print(ds[0]['instruction'] + ds[0]['input'] + ds[0]['output'] + tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636a8e7-0b1e-4639-b0ed-a4cef1a4c623",
   "metadata": {},
   "source": [
    "**方案(3)**:在方案2基础上更进一步,添加有效的区分信息."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de966d5-c2fe-4014-bb13-681897cb5b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 给定一个英文句子，翻译成中文。\n",
      "I love to learn new things every day.\n",
      "\n",
      "\n",
      "Assistant:我每天喜欢学习新事物。</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Human: \"+ ds[0]['instruction'] + ds[0]['input'] + \"\\n\\nAssistant:\" + ds[0]['output'] + tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404521e-e09f-47f2-8ea1-68d5561ba8ea",
   "metadata": {},
   "source": [
    "按照**方案(3)**的思路,我们可以构建数据处理函数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4535753e-0753-4e39-b2a6-a12be92e1ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def belle_open_source_500k(data_file, tokenizer, max_len):\n",
    "    # https://huggingface.co/datasets/BelleGroup/train_0.5M_CN/blob/main/Belle_open_source_0.5M.json\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "                result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "                and len(result[\"input_ids\"]) < max_len\n",
    "                and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        if add_eos_token and len(result[\"input_ids\"]) >= max_len:\n",
    "            result[\"input_ids\"][max_len - 1] = tokenizer.eos_token_id\n",
    "            result[\"attention_mask\"][max_len - 1] = 1\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        instruction = data_point['instruction']\n",
    "        input_text = data_point[\"input\"]\n",
    "        input_text = \"Human: \" + instruction + input_text + \"\\n\\nAssistant: \"\n",
    "        input_text = tokenizer.bos_token + input_text if tokenizer.bos_token != None else input_text\n",
    "        target_text = data_point[\"output\"] + tokenizer.eos_token\n",
    "        full_prompt = input_text + target_text\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    data = load_dataset(\"json\", data_files=data_file)[\"train\"]\n",
    "    data = data.map(generate_and_tokenize_prompt, num_proc=8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "471ccbef-fce3-44a1-88af-6c8323e53256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/kky/.cache/huggingface/datasets/json/default-46b81080a00b6e53/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 67.26it/s]\n",
      "                                                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "ds = belle_open_source_500k(\"./data/Belle_open_source_0.5M.json\", tokenizer, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "156fa2a1-327b-4d16-aa83-1da75fd775a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5132, 31143, 31106, 31423, 31261, 1197, 15215, 22667, 72]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c828088e-4ff5-4528-aaac-37eca55f6021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Human: 给定一个英文句子，翻译成中文。\\nI love to learn new things every day.\\n\\n\\nAssistant: 我每天喜欢学习新事物。</s>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ebaa4-70d2-490e-b03b-90cb9fefec3f",
   "metadata": {},
   "source": [
    "## 多轮对话场景\n",
    "\n",
    "多轮对话与单轮对话的区别在于, 有多个轮次的交互信息,一些模型比如`ChatGLM`将轮次信息写入到上下文,让模型能够获得对多轮对话更有层次的信息利用.\n",
    "\n",
    "```python\n",
    "\n",
    "def build_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):\n",
    "    prompt = \"\"\n",
    "    for i, (old_query, response) in enumerate(history):\n",
    "        prompt += \"[Round {}]\\n\\n问：{}\\n\\n答：{}\\n\\n\".format(i + 1, old_query, response)\n",
    "    prompt += \"[Round {}]\\n\\n问：{}\\n\\n答：\".format(len(history) + 1, query)\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(self.device)\n",
    "    return inputs\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e85b28-41e2-43bf-b356-577d6dc5a6af",
   "metadata": {},
   "source": [
    "## Label Mask\n",
    "\n",
    "相信一些同学看到这里已经有一些想法,我们其实希望模型能够有效的回答问题,但是如果按照我们上面的数据处理方式,模型其实还在预测:\n",
    "1. 额外添加的辅助信息,比如轮次等\n",
    "2. 人类提问\n",
    "\n",
    "因此我们可以定义一个`Label Mask`让模型计算loss的时候,不考虑这些token,专注与训练模型回答."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439898e-0f53-4d99-818f-dd9f0c8945d0",
   "metadata": {},
   "source": [
    "# 模型\n",
    "\n",
    "## 量化加载\n",
    "- baichuan-7B显存开销 `float16` (13GB), `8bit` (7.1GB), `4bit` (4.1GB)\n",
    "- 模型定义变化 `torch.nn.Linear` => `bitsandbytes.nn.Linear8bitLt` => `bitsandbytes.nn.Linear4bit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7ae049-2d3b-4689-b974-ae15d31c62ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/torch2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1130cbf6-3254-472b-ae4e-5edaee879be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage of model: 1.3e+01 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"./pretrained/baichuan-7b/\",\n",
    "            device_map={\"\": 0},\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "print(f'memory usage of model: {model.get_memory_footprint() / (1024 * 1024 * 1024):.2} GB')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebcc45f-66df-4d51-829c-567e93963490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/kky/miniconda3/envs/torch2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/kky/miniconda3/envs/torch2/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/kky/miniconda3/envs/torch2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "memory usage of model: 7.1 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear8bitLt(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"./pretrained/baichuan-7b/\",\n",
    "            device_map={\"\": 0},\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_threshold=6.0\n",
    "            )\n",
    "        )\n",
    "print(f'memory usage of model: {model.get_memory_footprint() / (1024 * 1024 * 1024):.2} GB')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a861ca-2feb-4056-99dc-0c320263e2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage of model: 4.1 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"./pretrained/baichuan-7b/\",\n",
    "            device_map={\"\": 0},\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "print(f'memory usage of model: {model.get_memory_footprint() / (1024 * 1024 * 1024):.2} GB')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d11532-a0e4-4f64-9585-132cc7247f38",
   "metadata": {},
   "source": [
    "## 找到需要Lora finetune的Module Names\n",
    "\n",
    "1. 如果已经进行了量化加载,可以通过模型类进行寻找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c52203-49dc-4917-86fe-39a10d892df8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down_proj', 'o_proj', 'W_pack', 'gate_proj', 'up_proj']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "lora_module_names = set()\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, bnb.nn.Linear4bit):\n",
    "        names = name.split('.')\n",
    "        lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "if 'lm_head' in lora_module_names: # needed for 16-bit or 32-bit\n",
    "    lora_module_names.remove('lm_head')\n",
    "\n",
    "list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f04a1-e000-48c5-ab01-a712f7aec630",
   "metadata": {},
   "source": [
    "2. 根据模型Module名称手动指定\n",
    "```python\n",
    "lora_module_names = [\"W_pack\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fea1d-59b0-4243-bb38-f561a5536397",
   "metadata": {},
   "source": [
    "## Label shift and Loss\n",
    "\n",
    "语言模型一般采用`CrossEntropyLoss`,需要注意的是一般会设置`ignore_index=tokenizer.pad_token_id`,这个在`Transformers`库中的模型已经内置实现了. 另外在模型`forward`过程中会自动的对`input_ids`进行`shift`1位的操作得到`labels`. 下面代码为`transformers`库源码,其中输入的`input_ids`和`labels`是完全一致的,但是模型需要预测的是下一个单词,因此将`labels`进行`shift`操作即可得到我们需要的真正`labels`.\n",
    "\n",
    "\n",
    "```python\n",
    "# https://huggingface.co/baichuan-inc/baichuan-7B/blob/5bcc98cbecaee775d90c9a168cc526bbabcf0983/modeling_baichuan.py#L612\n",
    "if labels is not None:\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        shift_labels = shift_labels.to(shift_logits.device)\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f1138-173d-4d87-b546-be6bd7334eb0",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "`transformers`内置的**Trainer**已经实现了几乎完整的语言模型训练功能,在实践时一般只需要对`compute_loss`方法进行重写即可, 例如:\n",
    "\n",
    "```python\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "```\n",
    "\n",
    "**TrainingArguments**则定义了绝大部分训练需要的参数,和**Trainer**配套使用.主要参数:\n",
    "\n",
    "```latex\n",
    "num_train_epochs   迭代次数\n",
    "learning_rate      学习率\n",
    "per_device_train_batch_size  每个GPU batch_size, num_gpu * per_device_train_batch_size = 真正的batch_size\n",
    "gradient_accumulation_steps  累计梯度\n",
    "logging_steps                日志频率\n",
    "report_to                    日志存储,一般设置为wandb或者tensorboard\n",
    "optim                        优化器,一般设置adamw_torch\n",
    "ddp_find_unused_parameters   在使用DDP分布式训练时,一定要设这这个参数为false\n",
    "```\n",
    "\n",
    "\n",
    "**HfArgumentParser**可以很方便的代替`argpare`库,常常和**TrainingArguments**一起使用. 这里举例,首先定义一些其他需要传入的参数:\n",
    "\n",
    "```python\n",
    "from dataclasses import field, fields, dataclass\n",
    "\n",
    "@dataclass\n",
    "class FinetuneArguments:\n",
    "    model_name: str = field()\n",
    "    model_path: str = field()\n",
    "    data_name: str = field()\n",
    "    data_path: str = field()\n",
    "    train_size: int = field(default=-1)\n",
    "    test_size: int = field(default=200)\n",
    "    max_len: int = field(default=1024)\n",
    "    lora_rank: int = field(default=8)\n",
    "    lora_modules: str = field(default=None)\n",
    "    quantization: str = field(default=\"4bit\")\n",
    "\n",
    "# 这种定义中 arg_name: bool=filed(default=False)等价于 parser.add_argument(\"--arg_name\", action=\"store_true\")\n",
    "```\n",
    "\n",
    "然后所有参数一起解析:\n",
    "\n",
    "```python\n",
    "\n",
    "args, training_args = HfArgumentParser(\n",
    "        (FinetuneArguments, TrainingArguments)\n",
    "    ).parse_args_into_dataclasses()\n",
    "\n",
    "```\n",
    "\n",
    "这样所有参数都可以通过 `python script.py --model_name ....`方式传入, 其中属于`FinetuneArguments`的部分会被记录到`args`下, `TrainingArguments`记录到`training_args`下.下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba79bc5-7c68-4106-97ea-bcc270b545fd",
   "metadata": {},
   "source": [
    "# Finetune模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca55ad66-2068-4f89-b595-8634d98d9f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kky/miniconda3/envs/torch2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/kky/miniconda3/envs/torch2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/kky/miniconda3/envs/torch2/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/kky/miniconda3/envs/torch2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import datasets\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from model import load_model\n",
    "from peft import PeftModel\n",
    "from transformers import  GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75524a62-32f2-4baf-a917-dc832fb956a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model with 4bit quantization\n",
      "pass unk_token_id 0 to pad_token_id\n",
      "memory usage of model: 4.1 GB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"baichuan\", \"./pretrained/baichuan-7b/\", quantization=\"4bit\")\n",
    "lora_model = PeftModel.from_pretrained(model, \"./output/baichuan_lorasft/\")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "        temperature=0.5,\n",
    "        top_p = 0.85,\n",
    "        do_sample = True, \n",
    "        repetition_penalty=2.0, \n",
    "        max_new_tokens=1024,  # max_length=max_new_tokens+input_sequence\n",
    "\n",
    ")\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e07644-7268-4f42-93d7-fd50c48fca4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 我的名字是“小黑”，你可以考虑使用这个作为你的网络昵称。</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"帮我取一个很独特的网名\"\n",
    "input =\"Human: \" + prompt + \"\\n\\nAssistant: \"\n",
    "inputs = tokenizer(input,return_tensors=\"pt\").to(device)\n",
    "generate_ids = lora_model.generate(**inputs, generation_config=generation_config)\n",
    "output = tokenizer.decode(generate_ids[0][len(inputs.input_ids[0]):])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a187476-6c17-446e-a96c-90f7f41756b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「Unknown and infinite possibilities」\n"
     ]
    }
   ],
   "source": [
    "prompt = \"这段话翻译成英文: '代表未知、无限可能'\"\n",
    "input =\"Human: \" + prompt + \"\\n\\nAssistant: \"\n",
    "inputs = tokenizer(input,return_tensors=\"pt\").to(device)\n",
    "generate_ids = lora_model.generate(**inputs, generation_config=generation_config)\n",
    "output = tokenizer.decode(generate_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae97dcdb-88e8-41a3-bc1c-2a613c799c80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「月夜之舞」\n"
     ]
    }
   ],
   "source": [
    "prompt = \"帮我取一个很月亮有关且很独特的网名\"\n",
    "input =\"Human: \" + prompt + \"\\n\\nAssistant: \"\n",
    "inputs = tokenizer(input,return_tensors=\"pt\").to(device)\n",
    "generate_ids = lora_model.generate(**inputs, generation_config=generation_config)\n",
    "output = tokenizer.decode(generate_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffceeec-bbd2-40da-99a3-689834756cea",
   "metadata": {},
   "source": [
    "**祝大家玩的愉快! 更详细的LLM视频更新在我的视频媒体上,欢迎关注.**\n",
    "- b站频道: https://space.bilibili.com/1751715710/channel/collectiondetail?sid=1485775\n",
    "- youtube: https://www.youtube.com/channel/UCxu8MUtWtqfdDB2eTABSePg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
